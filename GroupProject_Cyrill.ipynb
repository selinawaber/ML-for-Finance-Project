{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# House Prices Group 5 \n",
    "# (Aleksandar, Arthur, Cyrill, Selina)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a041f",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f4190",
   "metadata": {},
   "source": [
    "# Function for Printing and Showing Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_crossvalidation(func, X_test, y_test):\n",
    "  \n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_:}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score:}\")\n",
    "  print(\"Test Score:\".format(func.score(X_test, y_test)))\n",
    "\n",
    "def report(y_true, y_pred):\n",
    "    \n",
    "  class_report = metrics.classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  conf_matrix = confusion_matrix(y_true, y_pred, normalize = \"all\")\n",
    "  conf_matrix = pd.DataFrame(conf_matrix, [\"Class 0\", \"Class 1\", \" Class 2\", \"Class 3\", \" Class 4\"],  [\"Class 0\", \"Class 1\", \" Class 2\", \"Class 3\", \" Class 4\"])\n",
    "  sns.heatmap(conf_matrix, annot = True).set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed8e63",
   "metadata": {},
   "source": [
    "## Load data and initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae83ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54087106",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ded341",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "df_nan.isna().sum() / df_nan.shape[0]\n",
    "\n",
    "# Plot missing values 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a3708",
   "metadata": {},
   "source": [
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the [data description](https://www.openml.org/search?type=data&sort=runs&id=42165&status=active).\n",
    "\n",
    "The following variables have NAs that can be filled:\n",
    "\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values for variables where appropriate\n",
    "\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"No\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"No\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"No\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"No\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"No\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"No\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"No\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"No\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"No\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"No\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd2139",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt. \n",
    "\n",
    "- LotFrontage ---> High Correlation with other variable?\n",
    "- GarageYrBlt can probably be ignored since it highly correlates with YearBuilt. \n",
    "- MasVnrType and MasVnrArea have a strong correaltion with \"YearBuilt\" and \"OverallQual\" ---> Delete them?\n",
    "- Electrical one missing value ---> Delete this observation or just leave it?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data cleaning\n",
    "df = df.dropna(axis='columns', thresh=1459)\n",
    "df = df.dropna(axis='rows', how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a75add",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Dealing with Categorical Features (Encoding Categorical Variables) / Splitting Into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that MoSold was succesfully changed\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d39396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ee2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X and response vector y\n",
    "X = df.iloc[:, 1:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26cce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorise the binary variables (no need to create two dummy variables)\n",
    "# ---> Problem of Multicollinearity \n",
    "#Without this the get_dummies would create two variables CentralAir_y and CentralAir_n\n",
    "X[\"StreetFac\"] = X.Street.factorize()[0]\n",
    "X[\"CentralAirFac\"] = X.CentralAir.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "# panda way\n",
    "\n",
    "X = pd.get_dummies(X.iloc[:,:]) # not using ID\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c805f453",
   "metadata": {},
   "source": [
    "### Partitioning of the Data Set Into Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc69cf",
   "metadata": {},
   "source": [
    "We are using a 70/30 (training/testing) splitting. (The parameter `random_state=0` fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55baf251",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e11a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820e666",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = IsolationForest(random_state = 42).fit(X_train)\n",
    "out_train = out.predict(X_train)\n",
    "out_test = out.predict(X_test)\n",
    "\n",
    "\n",
    "X_train_wout_out = X_train[np.where(out_train == 1, True, False)]\n",
    "y_train_wout_out = y_train[np.where(out_train == 1, True, False)]\n",
    "X_test_wout_out = X_test[np.where(out_test == 1, True, False)]\n",
    "y_test_wout_out = y_test[np.where(out_test == 1, True, False)]\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"Shape including outliers: \", X_train.shape)\n",
    "print(\"Shape excluding outliers: \", X_train_wout_out.shape)\n",
    "print(\"Nr. of outliers removed: \", X_train.shape[0]-X_train_wout_out.shape[0])\n",
    "\n",
    "print(50*\"-\")\n",
    "\n",
    "print(\"Test Set\")\n",
    "print(\"Shape including outliers: \", X_test.shape)\n",
    "print(\"Shape excluding outliers: \", X_test_wout_out.shape)\n",
    "print(\"Nr. of outliers removed: \", X_test.shape[0]-X_test_wout_out.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2298b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data without the outliers for the models\n",
    "\n",
    "# Traning Set\n",
    "X_train = X_train_wout_out\n",
    "y_train = y_train_wout_out\n",
    "\n",
    "# Test Set\n",
    "X_test = X_test_wout_out\n",
    "y_test = y_test_wout_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7557e7b",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30caa267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf939d3",
   "metadata": {},
   "source": [
    "## Assessing Target Variable \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7382179",
   "metadata": {},
   "source": [
    "** Assess Class imbalance. You make your own assessment on potential effects of class-imbalance. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0f6f0",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bcdfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e4bf7",
   "metadata": {},
   "source": [
    "# Decision Trees / Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456bd9f",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfdf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state = 42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Performance metrics for training and test set\n",
    "print('Train score: ', tree.score(X_train, y_train))\n",
    "print('Test score: ', tree.score(X_test, y_test))\n",
    "\n",
    "print(70*'-')\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Confusion matrix for Decision Tree: \\n')\n",
    "print(\"Left = Predicted; Top = Actual\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1095e",
   "metadata": {},
   "source": [
    "We see that 3 were predicted to be in class 4 but were actually in class 2. On was predicted to be in class 4 but was actually in class 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d1d41",
   "metadata": {},
   "source": [
    "### Visualizing Decision Tree\n",
    "[Scikit-learn website for details.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "plt.figure(figsize=(40, 23))\n",
    "plot_tree(tree, filled=True, feature_names = list(X.columns), rounded=True, class_names=[\"0\", \"1\", \"2\", \"3\", \"4\"],);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f9fc7",
   "metadata": {},
   "source": [
    "### Grid Search (Finding Hyperparameters)\n",
    "Evaluates the model performance for each combination of hyperparameter to obtain the optimal combination of values from this set (Raschka (2015))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all possible parameters\n",
    "print(f\"Parameters of the Decision Tree: {tree.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba259d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Fold CV object (k = 5)\n",
    "kFold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "####### Parameters of the Decision Tree under investigation #######\n",
    "\n",
    "# In the initial tuning we included more values.\n",
    "# But more values cause more computational effort.\n",
    "# We only have a preselected list of values that include values around the best value.\n",
    "\n",
    "\n",
    "##### Estimators #####\n",
    "\n",
    "# Criterion\n",
    "# The function to measure the quality of a split.\n",
    "Criterion = np.array([\"gini\", \"entropy\"])\n",
    "\n",
    "# Splitter \n",
    "# The strategy used to choose the split at each node.\n",
    "Splitter = np.array([\"best\", \"random\"])\n",
    "\n",
    "# Class weight\n",
    "# Weights associated with classes in the form {class_label: weight}.\n",
    "class_Weight = np.array([None, \"balanced\", \"balanced_subsample\"])\n",
    "\n",
    "# Max depth\n",
    "# The maximum depth of the tree. \n",
    "maxDepth = np.array([1, 5, 7, 8, 9, 10, 11, 12, 15, 18])\n",
    "\n",
    "# Max features\n",
    "# The number of features to consider when looking for the best split.\n",
    "max_Features = np.array([None, \"auto\", \"sqrt\", \"log2\"])\n",
    "\n",
    "# min_Samples_Split\n",
    "# The minimum number of samples required to split an internal node.\n",
    "min_Samples_Split = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# minSamplesLeaf\n",
    "minSamplesLeaf = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b195e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter to be tested\n",
    "param_grid_tr = {\"class_weight\": class_Weight,\n",
    "              #\"criterion\": Criterion,\n",
    "              \"max_depth\": maxDepth,\n",
    "              #\"splitter\": Splitter,\n",
    "              #\"max_features\": max_Features,\n",
    "              \"min_samples_split\": min_Samples_Split,\n",
    "              \"min_samples_leaf\": minSamplesLeaf,\n",
    "}\n",
    "\n",
    "# grid search\n",
    "tree_gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42, max_features = \"sqrt\"),\n",
    "                  param_grid=param_grid_tr,\n",
    "                  scoring=\"accuracy\",\n",
    "                  cv=kFold, n_jobs=-1)\n",
    "tree_gs = tree_gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Performance of Decision Tree\")\n",
    "print_results_crossvalidation(tree_gs, X_test, y_test)\n",
    "y_pred = tree_gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee158ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take best parameter\n",
    "clf = tree_gs.best_estimator_\n",
    "\n",
    "# Fitting the model with the best parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out score on Test dataset\n",
    "print(\"Accuracy Test Set: {0: .4f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ebb236",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e78be",
   "metadata": {},
   "source": [
    "### Grid Search (Finding Hyperparameters)\n",
    "Evaluates the model performance for each combination of hyperparameter to obtain the optimal combination of values from this set (Raschka (2015))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Classifier object\n",
    "forest = RandomForestClassifier(n_estimators = 100, criterion=\"gini\", random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get a list of all parameters of random forest\n",
    "print(f\"Parameters of Random Forest: {forest.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c744d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Fold CV object (k = 5)\n",
    "kFold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "####### Parameters for the Random Forest #######\n",
    "\n",
    "# In the initial tuning we included more values.\n",
    "# But more values cause more computational effort.\n",
    "# We only have a preselected list of values that include values around the best value.\n",
    "\n",
    "\n",
    "##### Estimators #####\n",
    "\n",
    "# The number of trees in the forest.\n",
    "n_Estimators = np.array([90, 95, 100, 105])\n",
    "\n",
    "# Criterion\n",
    "# The function to measure the quality of a split.\n",
    "Criterion = np.array([\"gini\", \"entropy\"])\n",
    "\n",
    "# Class weight\n",
    "# Weights associated with classes in the form {class_label: weight}.\n",
    "class_Weight = np.array([None, \"balanced\", \"balanced_subsample\"])\n",
    "\n",
    "# Max depth\n",
    "# The maximum depth of the tree. \n",
    "maxDepth = np.array([5, 10, 15, 20])\n",
    "\n",
    "# Max features\n",
    "# The number of features to consider when looking for the best split.\n",
    "max_Features = np.array([None, \"auto\", \"sqrt\", \"log2\"])\n",
    "\n",
    "# min_Samples_Split\n",
    "# The minimum number of samples required to split an internal node.\n",
    "min_Samples_Split = np.array([2, 3, 4])\n",
    "\n",
    "# minSamplesLeaf\n",
    "minSamplesLeaf = np.array([1, 2, 3])\n",
    "\n",
    "# Bootstrap\n",
    "BootStrap = np.array([\"False\", \"True\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba306220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter to be tested (for computational reasons those where the default is the best are hashtaged)\n",
    "param_grid_fo = {\n",
    "              #\"class_weight\": class_Weight,\n",
    "              #\"criterion\": Criterion,\n",
    "              \"n_estimators\": n_Estimators,\n",
    "              \"max_depth\": maxDepth,\n",
    "              #\"max_features\": max_Features,\n",
    "              \"min_samples_split\": min_Samples_Split,\n",
    "              \"min_samples_leaf\": minSamplesLeaf,\n",
    "              #\"bootstrap\": BootStrap\n",
    "}\n",
    "\n",
    "# grid search\n",
    "forest_gs = GridSearchCV(estimator=RandomForestClassifier(random_state=0, n_jobs=-1, max_features = \"sqrt\"),\n",
    "                  param_grid=param_grid_fo,\n",
    "                  scoring=\"accuracy\",\n",
    "                  cv=kFold, n_jobs=-1)\n",
    "forest_gs = forest_gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Performance of Random Forest\")\n",
    "print_results_crossvalidation(forest_gs, X_test, y_test)\n",
    "y_pred = forest_gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take best parameter\n",
    "clf = forest_gs.best_estimator_\n",
    "\n",
    "# Fitting the model with the best parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out score on Test dataset\n",
    "print(\"Accuracy Test Set: {0: .4f}\".format(clf.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e63462479e65805acb443877c200ac6f9195cfa03c826fc8c678218149c104a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
