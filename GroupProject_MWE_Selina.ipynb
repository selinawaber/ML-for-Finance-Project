{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# Group Project: Minimal Working Expamle\n",
    "\n",
    "Below script is a minimum working example using the group project data to derive a ML-model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0b4e9",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b2a9d8",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbda7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values 2.0\n",
    "\n",
    "# Assess missing values\n",
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "print('Percentage of missing values per column:')\n",
    "df_nan.isna().sum() / df_nan.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# Plot missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assess Class imbalance. You make your own assessment on potential effects of class-imbalance.\n",
    "plt.hist(df['Class'], bins=[0, 1, 2, 3, 4, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66eff79",
   "metadata": {},
   "source": [
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the data description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc0125",
   "metadata": {},
   "source": [
    "The following variables have NAs that can be filled:\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc770bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values for variables where appropriate\n",
    "\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"None\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"None\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"None\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"None\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"None\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"None\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"None\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"None\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"None\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"None\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"None\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"None\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"None\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b345f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c7d76",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt.\n",
    "\n",
    "- LotFrontage ---> High Correlation with other variable?\n",
    "- GarageYrBlt can probably be ignored since it highly correlates with YearBuilt.\n",
    "- MasVnrType and MasVnrArea have a strong correaltion with \"YearBuilt\" and \"OverallQual\" ---> Delete them?\n",
    "- Electrical one missing value ---> Delete this observation or just leave it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62698f6",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628a580",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Features (Encoding Categorical Variables) / Splitting Into X and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e485f",
   "metadata": {},
   "source": [
    "# !!!!!!!!! Dropping NAs !!!!!!!!!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign response to y\n",
    "y = df.iloc[:, -1] # Corresponds to Class\n",
    "\n",
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "# panda way\n",
    "X = pd.get_dummies(df.iloc[:, :-1])\n",
    "X.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b328fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a9a2d",
   "metadata": {},
   "source": [
    "## Partitioning of the Data Set Into Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f706b8",
   "metadata": {},
   "source": [
    "We are using a 70/30 (training/testing) splitting. (The parameter random_state=0 fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5397ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf49c68",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector \n",
    " is a binary categorical variable with 25% zeros and 75% ones, stratify=y ensures that the random splits have 25% zeros and 75% ones too. Note that stratify=y does not mean stratify=yes but rather tells the function to take the categorical proportions from response vector y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012786c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_scl = X.columns.values\n",
    "cols_scl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc5ce8",
   "metadata": {},
   "source": [
    "## Feature Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee16c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:6]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "#X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "#X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "#X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "#X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f997b",
   "metadata": {},
   "source": [
    "# Assessing Target Variable \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002b9afd",
   "metadata": {},
   "source": [
    "** Assess Class imbalance. You make your own assessment on potential effects of class-imbalance. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3de280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a185062",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda0736",
   "metadata": {},
   "source": [
    "# !!! Vorgegeben Code !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_transformer, selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe369a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590209",
   "metadata": {},
   "source": [
    "Now let's see similarly for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__max_depth\": [1, 3, 5, 10],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_selector = selector(dtype_include=object)\n",
    "num_selector = selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    "    encoded_missing_value=-2,\n",
    ")\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n",
    ")\n",
    "\n",
    "#####\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", tree_preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10, 25],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2985fc3",
   "metadata": {},
   "source": [
    "# Copy Paste Function from Last Year !!!!!!\n",
    "\n",
    "For scoring our models we will be using the weighted \n",
    "$F_1$-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## Report Functions ##############\n",
    "# Function to get best parametrs, mean cross-validation score of best parameters, standard deviation of the cross-validation scores of the best parameters\n",
    "# and the score of the test set\n",
    "def get_results_cv(func, X_test_cleaned, y_test_cleaned):\n",
    "  \"\"\"\n",
    "  Inputs required: already fitted gridsearcv or randomsearchcv function, cleaned X test set, cleaned y test set\n",
    "  Returns best parameters, mean score, sd of score of cross-validation. Also returns test-score of best parameters\n",
    "  \"\"\"\n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_: .6f}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score: .6f}\")\n",
    "  print(\"Test Score: {:.6f}\".format(func.score(X_test_cleaned, y_test_cleaned)))\n",
    "\n",
    "# Function to get metrics report and heatmap of the confusion matrics for the test set\n",
    "def final_report(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Inputs required: true classes, predicted classes\n",
    "  Returns classification report and confusion matrix of the model\n",
    "  \"\"\"\n",
    "  class_report = metrics.classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  cm = confusion_matrix(y_true, y_pred, normalize = \"all\")\n",
    "  cm = pd.DataFrame(cm, [\"1\", \"2\", \"3\", \"4\", \"5\"],  [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "  plt.figure(figsize = (10,5))\n",
    "  sns.heatmap(cm, annot = True, fmt = \".2%\", cmap = \"Blues\").set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605052c",
   "metadata": {},
   "source": [
    "## Support Vector Machines (Selina)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0ca98",
   "metadata": {},
   "source": [
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a0d5e",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a type of supervised learning algorithm that can be utilized for tasks such as classification, regression, and outlier detection. One of the key benefits of SVMs is their effectiveness in handling high-dimensional data, as well as cases where the number of dimensions exceeds the number of samples. Additionally, SVMs are memory-efficient due to their use of a subset of training points, referred to as \"support vectors,\" in the decision-making process. Moreover, SVMs are highly versatile, as they can incorporate different kernel functions to specify the decision function, with the option to define custom kernels. It's important to acknowledge that SVMs come with their own set of limitations. One such limitation arises when dealing with datasets where the number of features far exceeds the number of samples; in such cases, it is essential to exercise caution in selecting kernel functions and regularization terms to prevent over-fitting. Another limitation of SVMs is that they do not offer direct probability estimates, which necessitates the use of a time-consuming five-fold cross-validation method to calculate them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46937ff",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa805025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd232a",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d00710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SVC Class from Sklearn\n",
    "svm= svm.SVC(\n",
    "        C=1.0,                          # The regularization parameter\n",
    "        kernel='rbf',                   # The kernel type used \n",
    "        degree=3,                       # Degree of polynomial function \n",
    "        gamma='scale',                  # The kernel coefficient\n",
    "        coef0=0.0,                      # If kernel = 'poly'/'sigmoid'\n",
    "        shrinking=True,                 # To use shrinking heuristic\n",
    "        probability=False,              # Enable probability estimates\n",
    "        tol=0.001,                      # Stopping crierion\n",
    "        cache_size=200,                 # Size of kernel cache\n",
    "        class_weight=None,              # The weight of each class\n",
    "        verbose=False,                  # Enable verbose output\n",
    "        max_iter= -1,                   # Hard limit on iterations\n",
    "        decision_function_shape='ovr',  # One-vs-rest or one-vs-one\n",
    "        break_ties=False,               # How to handle breaking ties\n",
    "        random_state=42               # Random state of the model\n",
    "    )\n",
    "\n",
    "print(f\"Parameters of the Support Vector Machine: {svm.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and training our model\n",
    "clf = svm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with our data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ba18",
   "metadata": {},
   "source": [
    "## Scaling/Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49084bed",
   "metadata": {},
   "source": [
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"SVM\", svm]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 10],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "get_results_cv(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "# get test score, metrics report and confusion matrix\n",
    "final_report(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48209d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
