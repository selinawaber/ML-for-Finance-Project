{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# Project ML in Finance Group 5\n",
    "### April 2023\n",
    "\n",
    "\n",
    "#### Cyrill Stoll, Arthur Schlegel, Aleksandar Kuljanin and Selina Waber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66ba9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dean De Cock created the Ames Housing dataset here the link to the [Dataset](https://www.openml.org/search?type=data&sort=runs&id=42165&status=active). This dataset provides information about the sales of residential properties in Ames, Iowa between 2006 and 2010. It consists of 2930 observations and includes a significant amount of explanatory variables, such as 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables, that are used to evaluate the values of homes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d908d81",
   "metadata": {},
   "source": [
    "## Importing Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# packages for decision tree / Random forest\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee1e9e",
   "metadata": {},
   "source": [
    "# Function for Printing and Showing Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_crossvalidation(func, X_test, y_test):\n",
    "  \n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_:}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score:}\")\n",
    "  print(\"Test Score:\".format(func.score(X_test, y_test)))\n",
    "\n",
    "def report(y_true, y_pred):\n",
    "    \n",
    "  class_report = metrics.classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  conf_matrix = confusion_matrix(y_true, y_pred, normalize = \"all\")\n",
    "  conf_matrix = pd.DataFrame(conf_matrix, [\"Class 0\", \"Class 1\", \" Class 2\", \"Class 3\", \" Class 4\"],  [\"Class 0\", \"Class 1\", \" Class 2\", \"Class 3\", \" Class 4\"])\n",
    "  sns.heatmap(conf_matrix, annot = True).set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4cc8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',', index_col='Id')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a1591",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2edbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = list(df.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\", numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = list(df.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894266b",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Identifying missing values in data is crucial before determining the appropriate course of action, such as dropping features or imputing missing values, as many machine learning algorithms generate errors when trained on incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess missing values\n",
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "\n",
    "\n",
    "# Plot missing values 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a38e",
   "metadata": {},
   "source": [
    "### Filling missing values for variables where appropriate\n",
    "\n",
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the data description.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a8ba4",
   "metadata": {},
   "source": [
    "#### Filling Categorical Variables\n",
    "\n",
    "The following variables have NAs that can be filled:\n",
    "\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement\n",
    "- MasVnrType: NA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling Categorical Variables \n",
    "\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"No\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"No\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"No\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"No\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"No\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"No\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"No\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"No\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"No\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"No\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"No\")\n",
    "df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(value= \"No\") #newly added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf59c00",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4453c5c",
   "metadata": {},
   "source": [
    "#### Filling missing values for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c90e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical Variables\n",
    "missing_numerical = ['GarageArea', 'GarageCars', 'BsmtFinSF1',\n",
    "                     'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF',\n",
    "                     'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "df[missing_numerical] = df[missing_numerical].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617aa86",
   "metadata": {},
   "source": [
    "#### Filling special variables\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling special variables\n",
    "\n",
    "df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(df[\"YearBuilt\"]) \n",
    "# assuming that the garge was bulit with the house \n",
    "\n",
    "\n",
    "df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "most_frequent= df['Electrical'].value_counts().idxmax()\n",
    "df[\"Electrical\"] = df[\"Electrical\"].fillna(most_frequent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436a1c0",
   "metadata": {},
   "source": [
    "## Outliers - To DO -\n",
    "\n",
    "Because regression models are very sensitive to outlier, we need to be aware of them. In the case of categorical data one can use sklearn's `OneHotEncoder` and specify the `min_frequency` parameter. If you specified the min_frequency parameter, rare categorical values will be assigned `infrequend_sklearn`.\n",
    "\n",
    "https://medium.com/owl-analytics/categorical-outliers-dont-exist-8f4e82070cb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7a954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48671e08",
   "metadata": {},
   "source": [
    "## Create New Variables\n",
    "\n",
    "<mark> Copy pastet from here https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> \n",
    "    \n",
    "Should maybe change that and not copy paste it ???\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0931ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totalSqFeet'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "df['totalBathroom'] = df[\"FullBath\"] + df[\"BsmtFullBath\"] + 0.5 * (df[\"HalfBath\"] + df[\"BsmtHalfBath\"])\n",
    "df['houseAge'] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
    "df['reModeled'] = np.where(df[\"YearRemodAdd\"] == df[\"YearBuilt\"], 0, 1)\n",
    "df['isNew'] = np.where(df[\"YrSold\"] == df[\"YearBuilt\"], 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd0e5",
   "metadata": {},
   "source": [
    "Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b948f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_anymore = ['TotalBsmtSF','1stFlrSF', '2ndFlrSF',\n",
    "                    \"FullBath\", \"BsmtFullBath\", \"HalfBath\",\n",
    "                    \"BsmtHalfBath\", \"YearBuilt\", \"YearRemodAdd\"  ]\n",
    "\n",
    "df= df.drop(not_used_anymore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d126ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653a23d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "### Dealing with Categorical Features (Encoding Categorical Variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71930fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other approach:\n",
    "#to_factor_cols = ['YrSold', 'MoSold', 'MSSubClass']\n",
    "#for col in to_factor_cols:\n",
    "#    X[col] = X[col].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0532dae",
   "metadata": {},
   "source": [
    "## Numerical Features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bdf6a",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1054fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize data to gain insights (Histograms)\n",
    "df.hist(figsize=(30, 20), bins = 15, edgecolor = 'black', grid = False, color = 'royalblue')\n",
    "plt.suptitle('Histograms of numerical features', x = 0.5, y = 1.02, size = 35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2e5a",
   "metadata": {},
   "source": [
    "#### Top 10 numerical variables highly correlated with `Class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_mat = df.corr(numeric_only=False).Class.sort_values(ascending=False)\n",
    "#corr_mat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1be1f",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "\n",
    "What are the top 10 features selected by Recursive Feature Elimination?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X interim and response vector y interim\n",
    "X_interim = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_interim , X_test_interim, y_train_interim, y_test_interim = train_test_split(X_interim, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)\n",
    "\n",
    "frames = [X_train_interim, y_train_interim]\n",
    "df_train_interim = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f03244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "selector = rfe.fit(X_train_interim.fillna(0).select_dtypes(exclude='object'), y_train_interim)\n",
    "selectedFeatures = list(\n",
    "    X_interim.select_dtypes(exclude='object').columns[selector.support_])\n",
    "selectedFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458eedf",
   "metadata": {},
   "source": [
    " ??????? Can't be !!!!\n",
    " \n",
    " \n",
    " <mark>Can that be?<mark> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60086c2e",
   "metadata": {},
   "source": [
    "### Overall Quality\n",
    "\n",
    "Overall quality is a very important feature e.g. higher quality houses are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23877520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Overall Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b9e82",
   "metadata": {},
   "source": [
    "### Living Area\n",
    "\n",
    "The price of a house is linearly correlated with its living area. By examining the scatter plot depicted below, it is evident that there exist some outliers in the data, particularly the two houses positioned in the bottom-right corner. These houses have a living area of more than 4000 square feet but are priced lower than Class 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation: \", df[['GrLivArea','Class']].corr().iloc[1, 0])\n",
    "sns.jointplot(x=df['GrLivArea'],y= df['Class'], kind='reg', marginal_kws={'kde': True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf40709",
   "metadata": {},
   "source": [
    "### GarageCars\n",
    "\n",
    "houses with garage that can hold 4 cars are cheaper than houses with 3 garages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='GarageCars', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Garage Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35db40",
   "metadata": {},
   "source": [
    "### House Age\n",
    "\n",
    "In addition to living area, the age of a house also influences its price significantly. Typically, newer houses command higher prices on average. However, it is worth noting that there are some houses constructed before 1900 that have a relatively high price despite their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='houseAge', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Year Built')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde7a0c",
   "metadata": {},
   "source": [
    "## Lable Encoding ??? \n",
    "\n",
    "Ordinal categorical features are label encoded. ??? <mark> copy pastet from https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> does it even make sense??\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ordinal categorical columns\n",
    "label_encoding_cols = [\n",
    "    \"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"FireplaceQu\", \"Functional\",\n",
    "    \"GarageCond\", \"GarageQual\", \"HeatingQC\", \"KitchenQual\", \"LandSlope\",\n",
    "    \"LotShape\", \"PavedDrive\", \"PoolQC\", \"Street\", \"Utilities\"\n",
    "]\n",
    "\n",
    "# Apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in label_encoding_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039e437",
   "metadata": {},
   "source": [
    "## Asign columns to feature matrix X and response vector y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X and response vector y\n",
    "\n",
    "X = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"] \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c752",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd005622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorise the binary variables (no need to create two dummy variables)\n",
    "# ---> Problem of Multicollinearity \n",
    "#Without this the get_dummies would create two variables CentralAir_y and CentralAir_n\n",
    "X[\"StreetFac\"] = X.Street.factorize()[0]\n",
    "X[\"CentralAirFac\"] = X.CentralAir.factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "X = pd.get_dummies(X.iloc[:,:]) \n",
    "\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb459c",
   "metadata": {},
   "source": [
    "## Partitioning of the Data Set Into Train and Test Set\n",
    "\n",
    "We are using a 70/30 (training/testing) splitting. (The parameter `random_state=0` fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb796e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f3886",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector  ùë¶ is a binary categorical variable with 25% zeros and 75% ones, `stratify=y` ensures that the random splits have 25% zeros and 75% ones too. Note that `stratify=y` does not mean `stratify=yes` but rather tells the function to take the categorical proportions from response vector `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f205fef",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a91f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = IsolationForest(random_state = 42).fit(X_train)\n",
    "out_train = out.predict(X_train)\n",
    "out_test = out.predict(X_test)\n",
    "\n",
    "\n",
    "X_train_wout_out = X_train[np.where(out_train == 1, True, False)]\n",
    "y_train_wout_out = y_train[np.where(out_train == 1, True, False)]\n",
    "X_test_wout_out = X_test[np.where(out_test == 1, True, False)]\n",
    "y_test_wout_out = y_test[np.where(out_test == 1, True, False)]\n",
    "\n",
    "print(\"Training Set\")\n",
    "print(\"Shape including outliers: \", X_train.shape)\n",
    "print(\"Shape excluding outliers: \", X_train_wout_out.shape)\n",
    "print(\"Nr. of outliers removed: \", X_train.shape[0]-X_train_wout_out.shape[0])\n",
    "\n",
    "print(50*\"-\")\n",
    "\n",
    "print(\"Test Set\")\n",
    "print(\"Shape including outliers: \", X_test.shape)\n",
    "print(\"Shape excluding outliers: \", X_test_wout_out.shape)\n",
    "print(\"Nr. of outliers removed: \", X_test.shape[0]-X_test_wout_out.shape[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896df82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the data without the outliers for the models\n",
    "\n",
    "# Traning Set\n",
    "X_train = X_train_wout_out\n",
    "y_train = y_train_wout_out\n",
    "\n",
    "# Test Set\n",
    "X_test = X_test_wout_out\n",
    "y_test = y_test_wout_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f08d7",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7484c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Selection##\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Plotting Heatmap\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.heatmap(corr, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48051ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Sort the correlations with respect to 'Class'\n",
    "corr_with_class = corr_matrix['Class'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlations\n",
    "print(corr_with_class)\n",
    "\n",
    "# Select the top 10 features with the highest correlation\n",
    "top_features = corr_with_class.nlargest(10).index\n",
    "\n",
    "# Print the top features\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "##VIF## \n",
    "\n",
    "# load the data and select features\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
    "        'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "\n",
    "# add a constant to X for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# calculate VIF for each feature\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "# print the VIF table\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e0935",
   "metadata": {},
   "source": [
    "## Skewness and Normalizing Variables\n",
    "\n",
    "Linear regression assumes that the data follows a normal distribution, and therefore, transforming skewed data can improve the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc18e1",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively/right-skewed skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e14ee",
   "metadata": {},
   "source": [
    "To normalize right-skewed data, log transformation can be used as a method since it pulls the larger values towards the center. However, because log(0) results in NaN, log(1+X) is preferred as a fix for the skewness instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=y, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d85b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trafo = np.log(1 + y)\n",
    "sns.displot(data=y_trafo, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution Transformation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e87a8d",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e927373",
   "metadata": {},
   "source": [
    "Standardizing the dataset before running machine learning algorithms is generally recommended, except for Decision Tree and Random Forest models. This is because optimization methods and gradient descent algorithms tend to perform and converge faster on features that are similarly scaled.\n",
    "\n",
    "However, outliers can have a negative impact on the sample mean and standard deviation, and models like Lasso and others are highly sensitive to outliers. In such cases, using the median and interquartile range is a better alternative. For this reason, the <mark> RobustScaler?? or StandardScaler? <mark> method is used to transform the training data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324bc53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ad823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a3e25c3",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f4758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling with MinMaxScalar (hence the model will perform better)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm_MMS = mms.fit_transform(X_train)  # fit & transform\n",
    "X_test_norm_MMS  = mms.transform(X_test)  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69deb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling with StandartScalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_norm_STD = scaler.transform(X_train)\n",
    "X_test_norm_STD = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model (using MinMaxStandardization)\n",
    "ConvergenceWarning(\"ignore\")\n",
    "\n",
    "lr = LogisticRegression().fit(X_train_norm_MMS,y_train)\n",
    "y_pred = lr.predict(X_test_norm_MMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred) #Report of LogReg using all features and MinMaxStandardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021843dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametrization of Logistic Regression (using MinMaxStandardization)\n",
    "\n",
    "# Muting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#hyperparameters\n",
    "param_grid_lr_MMS = {\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # Algorithm to use in the optimization problem.\n",
    "    \"penalty\": [\"l1\",\"l2\"], #specify norm of penalty\n",
    "    \"C\": np.logspace(-3,3,15), #smaller values specify stronger regularization    \n",
    "}\n",
    "# Run brute-force grid search\n",
    "lr_grid = GridSearchCV(estimator=LogisticRegression(), #model\n",
    "                             param_grid=param_grid_lr_MMS, #hyperparameters\n",
    "                             verbose=1, \n",
    "                             cv=10, #number of folds\n",
    "                             n_jobs=-1) #using all cores of computer\n",
    "# fitting the model\n",
    "lr_grid.fit(X_train_norm_MMS, y_train)\n",
    "y_pred = lr_grid.predict(X_test_norm_MMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651828f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred) #Report of LogReg using all features, MinMaxStandardization and Hyperparametrization of usual LogReg-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model (using StandardScalar)\n",
    "lr = LogisticRegression().fit(X_train_norm_STD,y_train)\n",
    "y_pred = lr.predict(X_test_norm_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred) #Report of LogReg using StandartScalar Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a26e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametrization of Logistic Regression (using StandardScalar)\n",
    "\n",
    "# Muting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#hyperparameters\n",
    "param_grid_lr_STD = {\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # Algorithm to use in the optimization problem.\n",
    "    \"penalty\": [\"l1\",\"l2\"], #specify norm of penalty\n",
    "    \"C\": np.logspace(-3,3,15), #smaller values specify stronger regularization    \n",
    "}\n",
    "# Run brute-force grid search\n",
    "lr_grid = GridSearchCV(estimator=LogisticRegression(), #model\n",
    "                             param_grid=param_grid_lr_STD, #hyperparameters\n",
    "                             verbose=1, \n",
    "                             cv=10, #number of folds\n",
    "                             n_jobs=-1) #using all cores of computer\n",
    "# fitting the model\n",
    "lr_grid.fit(X_train_norm_STD, y_train)\n",
    "\n",
    "y_pred = lr_grid.predict(X_test_norm_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f725074",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred) #Report of LogReg usuing StandartScalar Standardization and Hyperparametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Random Forest Feature Selection for Logistic Regression##########################################\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "sel.fit(X_train_norm_STD,y_train)\n",
    "sel.get_support\n",
    "selected_feat= X_train.columns[(sel.get_support())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a74d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set with just Random Forest Features\n",
    "X_train_rf = X_train[selected_feat]\n",
    "X_test_rf = X_test[selected_feat]\n",
    "\n",
    "#Feature Scaling using StandardScalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_rf)\n",
    "X_train_rf_STD = scaler.transform(X_train_rf)\n",
    "X_test_rf_STD= scaler.transform(X_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "logModel_rf = LogisticRegression().fit(X_train_rf_STD,y_train)\n",
    "y_pred_rf = logModel_rf.predict(X_test_rf_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e189641",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred_rf) #Report of LogReg usuing StandartScalar Standardization and Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3aea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametrization on Model using Random Forest Feature Selection\n",
    "# Muting warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#hyperparameters\n",
    "param_grid_lr_rf = {\n",
    "    \"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], # Algorithm to use in the optimization problem.\n",
    "    \"penalty\": [\"l1\",\"l2\"], #specify norm of penalty\n",
    "    \"C\": np.logspace(-3,3,15), #smaller values specify stronger regularization    \n",
    "}\n",
    "\n",
    "lr_rf_grid = GridSearchCV(estimator=LogisticRegression(),\n",
    "                          param_grid=param_grid_lr_rf,\n",
    "                          cv = 10,\n",
    "                          n_jobs=-1\n",
    "                          )\n",
    "lr_rf_grid.fit(X_train_rf_STD,y_train)\n",
    "y_pred_rf = logModel_rf.predict(X_test_rf_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report(y_test,y_pred) #Report of LogReg using StandartScalar Standardization, Random Forest Feature Selection and Hyperparam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5a4f85b",
   "metadata": {},
   "source": [
    "#### Summary on Logistic Regression\n",
    "\n",
    "#### Using MinMaxScalar \n",
    "We find the following Scores:\n",
    "f1 Score:         0.78\n",
    "\n",
    "#### Using MinMaxScalar and hyperparametrization\n",
    "We find the following Scores:\n",
    "f1 Score:         0.81\n",
    "\n",
    "#### Using StandardScalar\n",
    "We find the following Scores:\n",
    "f1 Score:         0.76\n",
    "\n",
    "#### Using StandardScalar and hyperparametrization \n",
    "We find the following Scores:\n",
    "f1 Score:         0.82\n",
    "\n",
    "#### Using StandardScaler and  RandomForest Feature Selection \n",
    "We find the following Scores:\n",
    "f1 Score:         0.83\n",
    "\n",
    "#### Using StandardScaler, RandomForest Feature Selection and Hyperparametrization \n",
    "We find the following Scores:\n",
    "f1 Score:         0.82\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55532c6",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02aabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN-Model##\n",
    "train_df = pd.DataFrame(data=X_train, columns=X.columns)\n",
    "train_df['Class'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(data=X_test, columns=X.columns)\n",
    "test_df['Class'] = y_test\n",
    "\n",
    "X_train = train_df[['Class', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "y_train = train_df['Class']\n",
    "\n",
    "# Initialize the KNN classifier with k=20\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit the KNN model on the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained KNN model to make predictions on the test data\n",
    "X_test = test_df[['Class', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "y_test = test_df['Class']\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the KNN model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute the classification report for the KNN model on the test data\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f370e",
   "metadata": {},
   "source": [
    "The model correctly classified 75% of the instances in the test set.\n",
    "The report indicates that the model performs well in predicting class 1, with a precision of 0.82 and a recall of 0.90, indicating that it correctly predicted a high proportion of the instances belonging to this class. However, the model performed less well in predicting classes 0 and 3, with a precision and recall of around 0.5. The model had moderate performance for class 2, with a precision and recall of around 0.6, and poor performance for class 4, with a precision of 0.5 and a recall of 0.38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3831c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of values for k\n",
    "k_range = range(1, 80)\n",
    "\n",
    "# Create an empty list to store the cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation for each value of k\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores.append(np.mean(scores))\n",
    "\n",
    "# Plot the cross-validation scores as a function of k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(k_range, cv_scores)\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Select the best value of k\n",
    "best_k = np.argmax(cv_scores) + 1\n",
    "print(\"The best value of k is:\", best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d0d0e",
   "metadata": {},
   "source": [
    "We can set k=20 in our KNN model to get the best possible performance on this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9b2f1",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###KNN-SMOTE####\n",
    "# Create SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Fit SMOTE on the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the number of samples before and after SMOTE\n",
    "print(\"Number of samples before SMOTE:\", len(X_train))\n",
    "print(\"Number of samples after SMOTE:\", len(X_train_smote))\n",
    "\n",
    "# Train a KNN classifier on the oversampled data\n",
    "knn_smote = KNeighborsClassifier(n_neighbors=20)\n",
    "knn_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict the test set using the KNN classifier trained on the oversampled data\n",
    "y_pred_smote = knn_smote.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the KNN model with SMOTE on the test data\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "print(\"Accuracy with SMOTE:\", accuracy_smote)\n",
    "\n",
    "# Compute the classification report for the KNN model with SMOTE on the test data\n",
    "report_smote = classification_report(y_test, y_pred_smote)\n",
    "\n",
    "# Print the classification report\n",
    "print(report_smote)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb27276d",
   "metadata": {},
   "source": [
    "SMOTE is used to address class imbalance by creating synthetic samples. However, in this case, after oversampling with SMOTE, the performance of the KNN classifier on the test set actually got worse compared to the previous results without SMOTE. The accuracy dropped from 0.7534 to 0.621, and the overall classification report shows lower precision and recall scores for some classes. \n",
    "This could be due to overfitting on the training data or other factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498c6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Class Imbalance using Random Oversample\n",
    "# Initialize the random oversampler with random_state=0\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "# Resample the training data\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize the KNN classifier with k=20\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit the KNN model on the resampled training data\n",
    "knn.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Use the trained KNN model to make predictions on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the KNN model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with RandomOverSampler:\", accuracy)\n",
    "\n",
    "# Compute the classification report for the KNN model on the test data\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565515a9",
   "metadata": {},
   "source": [
    "RandomOverSampler technique to address the class imbalance in the training data. The accuracy of the model after using Random Oversampling is 0.589. The drop in accuracy could be due to the fact that the synthetic samples generated by Random Oversampling can add noise to the data, leading to overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d9bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "y = df['Class']\n",
    "\n",
    "# Create KNN object and run classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_prob = knn.predict_proba(X)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(len(set(y))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y, y_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "for i, color in zip(range(len(set(y))), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f7534",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature atrix X and target variable Y\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "Y = df['Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "# LDA-Modell\n",
    "lda = LDA(solver='lsqr')\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# performance\n",
    "print('default-rate: {0: .4f}'.format(np.sum(y_test)/len(y_test)))\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455df72f",
   "metadata": {},
   "source": [
    "In this specific case, the default rate is 1.29, which means that the most frequent class in the test set is Class 1. The F1-score is 0.79, which indicates that the model has good accuracy. The classification report shows precision, recall, and F1-score for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d937b",
   "metadata": {},
   "source": [
    "## Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2e1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###SMOTE###\n",
    "# Feature-Matrix X und Zielvariable Y ausw√§hlen\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "Y = df['Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "sm = SMOTE(random_state=0)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# LDA-Modell erstellen und anpassen\n",
    "lda = LDA(solver='lsqr')\n",
    "lda.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Vorhersagen auf Testdaten machen\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Konfusionsmatrix ausgeben\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559d569",
   "metadata": {},
   "source": [
    "Here, we have also tried to address the class imbalancing problem by applying SMOTE. But this also led to a deterioration of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11615b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "y = df['Class']\n",
    "\n",
    "# Create LDA object and run classifier\n",
    "lda = LDA(solver='lsqr')\n",
    "lda.fit(X, y)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_prob = lda.predict_proba(X)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(len(set(y))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y, y_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "for i, color in zip(range(len(set(y))), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33681845",
   "metadata": {},
   "source": [
    "# QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80312507",
   "metadata": {},
   "outputs": [],
   "source": [
    "####QDA\n",
    "# Feature-Matrix X und Zielvariable Y ausw√§hlen\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "Y = df['Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# QDA-Modell erstellen und anpassen\n",
    "qda = QDA()\n",
    "qda.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen auf Testdaten machen\n",
    "y_pred = qda.predict(X_test)\n",
    "\n",
    "# Konfusionsmatrix ausgeben\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4f0ce",
   "metadata": {},
   "source": [
    "The model achieves an accuracy of 0.82, and the precision and recall vary across the classes, with the highest precision and recall achieved for class 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc573513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df[['OverallQual', 'GrLivArea', 'GarageArea',\n",
    "       'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'FullBath', 'YearRemodAdd']]\n",
    "y = df['Class']\n",
    "\n",
    "# Create QDA object and run classifier\n",
    "qda = QDA()\n",
    "qda.fit(X, y)\n",
    "\n",
    "# Predict probabilities for each class\n",
    "y_prob = qda.predict_proba(X)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "for i in range(len(set(y))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y, y_prob[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "plt.figure()\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple']\n",
    "for i, color in zip(range(len(set(y))), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4d77d",
   "metadata": {},
   "source": [
    "# Decision Trees / Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f5d12",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56266d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing decision tree\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state = 42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Performance metrics for training and test set\n",
    "print('Train score: ', tree.score(X_train, y_train))\n",
    "print('Test score: ', tree.score(X_test, y_test))\n",
    "\n",
    "print(70*'-')\n",
    "\n",
    "# Confusion matrix\n",
    "y_pred = tree.predict(X_test)\n",
    "print('Confusion matrix for Decision Tree: \\n')\n",
    "print(\"Left = Predicted; Top = Actual\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c6f95",
   "metadata": {},
   "source": [
    "We see that 3 were predicted to be in class 4 but were actually in class 2. On was predicted to be in class 4 but was actually in class 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3ab77",
   "metadata": {},
   "source": [
    "### Visualizing Decision Tree\n",
    "[Scikit-learn website for details.](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06d6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tree\n",
    "plt.figure(figsize=(40, 23))\n",
    "plot_tree(tree, filled=True, feature_names = list(X.columns), rounded=True, class_names=[\"0\", \"1\", \"2\", \"3\", \"4\"],);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf74dc",
   "metadata": {},
   "source": [
    "### Grid Search (Finding Hyperparameters)\n",
    "Evaluates the model performance for each combination of hyperparameter to obtain the optimal combination of values from this set (Raschka (2015))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa970dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all possible parameters\n",
    "print(f\"Parameters of the Decision Tree: {tree.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Fold CV object (k = 5)\n",
    "kFold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "####### Parameters of the Decision Tree under investigation #######\n",
    "\n",
    "# In the initial tuning we included more values.\n",
    "# But more values cause more computational effort.\n",
    "# We only have a preselected list of values that include values around the best value.\n",
    "\n",
    "\n",
    "##### Estimators #####\n",
    "\n",
    "# Criterion\n",
    "# The function to measure the quality of a split.\n",
    "Criterion = np.array([\"gini\", \"entropy\"])\n",
    "\n",
    "# Splitter \n",
    "# The strategy used to choose the split at each node.\n",
    "Splitter = np.array([\"best\", \"random\"])\n",
    "\n",
    "# Class weight\n",
    "# Weights associated with classes in the form {class_label: weight}.\n",
    "class_Weight = np.array([None, \"balanced\", \"balanced_subsample\"])\n",
    "\n",
    "# Max depth\n",
    "# The maximum depth of the tree. \n",
    "maxDepth = np.array([1, 5, 7, 8, 9, 10, 11, 12, 15, 18])\n",
    "\n",
    "# Max features\n",
    "# The number of features to consider when looking for the best split.\n",
    "max_Features = np.array([None, \"auto\", \"sqrt\", \"log2\"])\n",
    "\n",
    "# min_Samples_Split\n",
    "# The minimum number of samples required to split an internal node.\n",
    "min_Samples_Split = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# minSamplesLeaf\n",
    "minSamplesLeaf = np.array([1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter to be tested\n",
    "param_grid_tr = {\"class_weight\": class_Weight,\n",
    "              #\"criterion\": Criterion,\n",
    "              \"max_depth\": maxDepth,\n",
    "              #\"splitter\": Splitter,\n",
    "              #\"max_features\": max_Features,\n",
    "              \"min_samples_split\": min_Samples_Split,\n",
    "              \"min_samples_leaf\": minSamplesLeaf,\n",
    "}\n",
    "\n",
    "# grid search\n",
    "tree_gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42, max_features = \"sqrt\"),\n",
    "                  param_grid=param_grid_tr,\n",
    "                  scoring=\"accuracy\",\n",
    "                  cv=kFold, n_jobs=-1)\n",
    "tree_gs = tree_gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Performance of Decision Tree\")\n",
    "print_results_crossvalidation(tree_gs, X_test, y_test)\n",
    "y_pred = tree_gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take best parameter\n",
    "clf = tree_gs.best_estimator_\n",
    "\n",
    "# Fitting the model with the best parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out score on Test dataset\n",
    "print(\"Accuracy Test Set: {0: .4f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bd89a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb9147",
   "metadata": {},
   "source": [
    "### Grid Search (Finding Hyperparameters)\n",
    "Evaluates the model performance for each combination of hyperparameter to obtain the optimal combination of values from this set (Raschka (2015))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075568a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Classifier object\n",
    "forest = RandomForestClassifier(n_estimators = 100, criterion=\"gini\", random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get a list of all parameters of random forest\n",
    "print(f\"Parameters of Random Forest: {forest.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7eae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Fold CV object (k = 5)\n",
    "kFold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "\n",
    "####### Parameters for the Random Forest #######\n",
    "\n",
    "# In the initial tuning we included more values.\n",
    "# But more values cause more computational effort.\n",
    "# We only have a preselected list of values that include values around the best value.\n",
    "\n",
    "\n",
    "##### Estimators #####\n",
    "\n",
    "# The number of trees in the forest.\n",
    "n_Estimators = np.array([90, 95, 100, 105])\n",
    "\n",
    "# Criterion\n",
    "# The function to measure the quality of a split.\n",
    "Criterion = np.array([\"gini\", \"entropy\"])\n",
    "\n",
    "# Class weight\n",
    "# Weights associated with classes in the form {class_label: weight}.\n",
    "class_Weight = np.array([None, \"balanced\", \"balanced_subsample\"])\n",
    "\n",
    "# Max depth\n",
    "# The maximum depth of the tree. \n",
    "maxDepth = np.array([5, 10, 15, 20])\n",
    "\n",
    "# Max features\n",
    "# The number of features to consider when looking for the best split.\n",
    "max_Features = np.array([None, \"auto\", \"sqrt\", \"log2\"])\n",
    "\n",
    "# min_Samples_Split\n",
    "# The minimum number of samples required to split an internal node.\n",
    "min_Samples_Split = np.array([2, 3, 4])\n",
    "\n",
    "# minSamplesLeaf\n",
    "minSamplesLeaf = np.array([1, 2, 3])\n",
    "\n",
    "# Bootstrap\n",
    "BootStrap = np.array([\"False\", \"True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter to be tested (for computational reasons those where the default is the best are hashtaged)\n",
    "param_grid_fo = {\n",
    "              #\"class_weight\": class_Weight,\n",
    "              #\"criterion\": Criterion,\n",
    "              \"n_estimators\": n_Estimators,\n",
    "              \"max_depth\": maxDepth,\n",
    "              #\"max_features\": max_Features,\n",
    "              \"min_samples_split\": min_Samples_Split,\n",
    "              \"min_samples_leaf\": minSamplesLeaf,\n",
    "              #\"bootstrap\": BootStrap\n",
    "}\n",
    "\n",
    "# grid search\n",
    "forest_gs = GridSearchCV(estimator=RandomForestClassifier(random_state=0, n_jobs=-1, max_features = \"sqrt\"),\n",
    "                  param_grid=param_grid_fo,\n",
    "                  scoring=\"accuracy\",\n",
    "                  cv=kFold, n_jobs=-1)\n",
    "forest_gs = forest_gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Performance of Random Forest\")\n",
    "print_results_crossvalidation(forest_gs, X_test, y_test)\n",
    "y_pred = forest_gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take best parameter\n",
    "clf = forest_gs.best_estimator_\n",
    "\n",
    "# Fitting the model with the best parameter\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print out score on Test dataset\n",
    "print(\"Accuracy Test Set: {0: .4f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fd53e",
   "metadata": {},
   "source": [
    "## !! UNDER CONSTRUCTION !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_transformer, selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe369a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590209",
   "metadata": {},
   "source": [
    "Now let's see similarly for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__max_depth\": [1, 3, 5, 10],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_selector = selector(dtype_include=object)\n",
    "num_selector = selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    "    encoded_missing_value=-2,\n",
    ")\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n",
    ")\n",
    "\n",
    "#####\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", tree_preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10, 25],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d4d70",
   "metadata": {},
   "source": [
    "# Support Vector Machines (Selina)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baad25c",
   "metadata": {},
   "source": [
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N ‚Äî the number of features) that distinctly classifies the data points.\n",
    "\n",
    "Support vector machines (SVMs) are a type of supervised learning algorithm that can be utilized for tasks such as classification, regression, and outlier detection. One of the key benefits of SVMs is their effectiveness in handling high-dimensional data, as well as cases where the number of dimensions exceeds the number of samples. Additionally, SVMs are memory-efficient due to their use of a subset of training points, referred to as \"support vectors,\" in the decision-making process. Moreover, SVMs are highly versatile, as they can incorporate different kernel functions to specify the decision function, with the option to define custom kernels. It's important to acknowledge that SVMs come with their own set of limitations. One such limitation arises when dealing with datasets where the number of features far exceeds the number of samples; in such cases, it is essential to exercise caution in selecting kernel functions and regularization terms to prevent over-fitting. Another limitation of SVMs is that they do not offer direct probability estimates, which necessitates the use of a time-consuming five-fold cross-validation method to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6531e7a",
   "metadata": {},
   "source": [
    "## Importing Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0097c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294d3ef",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "First, we will train our model by calling the standard SVC() function without doing Hyperparameter Tuning and see its classification and confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e399e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SVC Class from Sklearn\n",
    "svm1= svm.SVC(\n",
    "        C=1.0,                          # The regularization parameter\n",
    "        kernel='rbf',                   # The kernel type used \n",
    "        degree=3,                       # Degree of polynomial function \n",
    "        gamma='scale',                  # The kernel coefficient\n",
    "        coef0=0.0,                      # If kernel = 'poly'/'sigmoid'\n",
    "        shrinking=True,                 # To use shrinking heuristic\n",
    "        probability=False,              # Enable probability estimates\n",
    "        tol=0.001,                      # Stopping crierion\n",
    "        cache_size=200,                 # Size of kernel cache\n",
    "        class_weight=None,              # The weight of each class\n",
    "        verbose=False,                  # Enable verbose output\n",
    "        max_iter= -1,                   # Hard limit on iterations\n",
    "        decision_function_shape='ovr',  # One-vs-rest or one-vs-one\n",
    "        break_ties=False,               # How to handle breaking ties\n",
    "        random_state=42               # Random state of the model\n",
    "    )\n",
    "\n",
    "print(f\"Parameters of the Support Vector Machine: {svm1.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and training our model\n",
    "clf = svm1.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with our data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accurancy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\", precision_score(y_true= y_test, y_pred= predictions, average= 'weighted')) # WEIGHTED???? \n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\", recall_score(y_test, predictions, average= 'weighted'))  # WEIGHTED???? \n",
    "\n",
    "#Whole classification report\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18bb4c2",
   "metadata": {},
   "source": [
    "Notice that recall and precision for class 0, 3 and 4 are always 0. It means that the classifier is always classifying everything into class 1 and 2! This means our model needs to have its parameters tuned.\n",
    "Here is when the usefulness of GridSearch comes into the picture. We can search for parameters using GridSearch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b93d0",
   "metadata": {},
   "source": [
    "## Scaling/Pipeline\n",
    "\n",
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"SVM\", svm1]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 5, 10, 50],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c07cef",
   "metadata": {},
   "source": [
    "#### GridSearch\n",
    "\n",
    "SVM has hyper-parameters such as C or gamma values that can greatly impact its performance. However, determining the optimal hyper-parameters can be a difficult task. One way to do so is by exploring various combinations of hyper-parameters to identify the ones that work best.\n",
    "\n",
    "Often one uses Gridsearch, which involves creating a grid of hyper-parameters and trying out all possible combinations to determine the optimal set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1, refit = True, verbose = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e66009",
   "metadata": {},
   "source": [
    "The process performed by \"fit\" is more complex.\n",
    "Initially, it executes the same loop using cross-validation to identify the most optimal combination of parameters.\n",
    "After it determines the best combination, it proceeds to execute \"fit\" again on all the data that was passed to it (without implementing cross-validation). \n",
    "This second \"fit\" run is aimed at constructing a single new model utilizing the finest parameter setting identified in the first step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8523fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc659c0",
   "metadata": {},
   "source": [
    "You can inspect the best parameters found by GridSearchCV in the best_params_ attribute,\n",
    "and the best estimator in the best_estimator_ attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time gs.fit(X_train, y_train)\n",
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32a703",
   "metadata": {},
   "source": [
    "#### Precision, Recall and F1-Score\n",
    "\n",
    "Precision (positive predicted) is the fraction of relevant data among the retrieved data. On the other hand, recall (sensitivity) is the fraction of relevant data that were retrieved. Both precision and recall are based on relevance.\n",
    "\n",
    "\n",
    "In practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value.\n",
    "\n",
    "The column ‚Äúsupport‚Äù represents the number of samples that were present in each class of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b7c9a",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Rows represent the expected class labels, and columns represent the predicted class labels. To use the heatmap, it is wiser to use a normalized confusion matrix because the dataset may be imbalanced.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d935aa",
   "metadata": {},
   "source": [
    "### Random Forest Feature Selection ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "## Feature Selection using Random Forest for outside of pipeline\n",
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "sel.fit(X_train, y_train)\n",
    "sel.get_support()\n",
    "list_train_rf= X_train.columns[(sel.get_support())]\n",
    "list_test_rf= X_test.columns[(sel.get_support())]\n",
    "\n",
    "X_train_rf = X_train[list_train_rf]\n",
    "X_test_rf = X_test[list_test_rf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(sel.estimator_.feature_importances_.ravel()).hist()\n",
    "\n",
    "\n",
    "importances = sel.estimator_.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# X is the train data used to fit the model \n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2= svm.SVC(random_state = 42, max_iter = -1)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler],[\"ros\", ros], [\"SVM\", svm2]])\n",
    "\n",
    "param_grid= {\n",
    "    \"scaler\": [scaler, mms],\n",
    "    \"ros\": [ros, None],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 5, 10, 50],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train_rf, y_train)\n",
    "\n",
    "%time gs.fit(X_train_rf, y_train)\n",
    "print_results_crossvalidation(gs, X_test_rf, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test_rf)\n",
    "\n",
    "report(y_test, y_pred)\n",
    "print(\"Here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b35554",
   "metadata": {},
   "source": [
    "### XGBoost Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "## Feature Selection using XGBoost for outside of pipeline\n",
    "xgbc= xgb.XGBClassifier( objective = \"multi:softmax\", random_state = 42)\n",
    "select_xgbc = SelectFromModel(estimator = xgbc, threshold = \"median\")\n",
    "select_xgbc.fit(X_train, y_train)\n",
    "\n",
    "list_train_xgbc= X_train.columns[(select_xgbc.get_support())]\n",
    "list_test_xgbc= X_test.columns[(select_xgbc.get_support())]\n",
    "\n",
    "\n",
    "X_train_xgbc = X_train[list_train_xgbc]\n",
    "X_test_xgbc = X_test[list_test_xgbc]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm3 = svm.SVC(random_state = 42, max_iter = -1)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler],[\"ros\", ros], [\"SVM\", svm3]])\n",
    "\n",
    "param_grid= {\n",
    "    \"scaler\": [scaler, mms],\n",
    "    \"ros\": [ros, None],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 5, 10, 50],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train_xgbc, y_train)\n",
    "\n",
    "%time gs.fit(X_train_xgbc, y_train)\n",
    "print_results_crossvalidation(gs, X_test_xgbc, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test_xgbc)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab446e3",
   "metadata": {},
   "source": [
    "### PCA Dimension Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9766822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "svm4 = svm.SVC(random_state = 42, max_iter = -1, shrinking = True, kernel = \"sigmoid\", C = 1)\n",
    "pca = PCA()\n",
    "\n",
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca], [\"ros\", ros], [\"SVM\", svm4]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"],\n",
    "    \"pca__n_components\": np.arange(4, 10, 1)\n",
    "}\n",
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "%time gs.fit(X_train, y_train)\n",
    "print_results_crossvalidation(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8dbab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
