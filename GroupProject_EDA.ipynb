{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# Project ML in Finance Group 5\n",
    "### April 2023\n",
    "\n",
    "\n",
    "#### Cyrill Stoll, Arthur Schlegel, Aleksandar Kuljanin and Selina Waber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66ba9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dean De Cock created the Ames Housing dataset here the link to the [Dataset](https://www.openml.org/search?type=data&sort=runs&id=42165&status=active). This dataset provides information about the sales of residential properties in Ames, Iowa between 2006 and 2010. It consists of 2930 observations and includes a significant amount of explanatory variables, such as 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables, that are used to evaluate the values of homes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d908d81",
   "metadata": {},
   "source": [
    "## Importing Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4cc8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',', index_col='Id')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a1591",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2edbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = list(df.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\", numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = list(df.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894266b",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Identifying missing values in data is crucial before determining the appropriate course of action, such as dropping features or imputing missing values, as many machine learning algorithms generate errors when trained on incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess missing values\n",
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "\n",
    "\n",
    "# Plot missing values 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a38e",
   "metadata": {},
   "source": [
    "### Filling missing values for variables where appropriate\n",
    "\n",
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the data description.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a8ba4",
   "metadata": {},
   "source": [
    "#### Filling Categorical Variables\n",
    "\n",
    "The following variables have NAs that can be filled:\n",
    "\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement\n",
    "- MasVnrType: NA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling Categorical Variables \n",
    "\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"No\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"No\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"No\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"No\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"No\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"No\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"No\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"No\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"No\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"No\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"No\")\n",
    "df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(value= \"No\") #newly added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf59c00",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt.\n",
    "\n",
    "- LotFrontage ---> High Correlation with other variable?\n",
    "- GarageYrBlt can probably be ignored since it highly correlates with YearBuilt.\n",
    "- MasVnrType and MasVnrArea have a strong correaltion with \"YearBuilt\" and \"OverallQual\" ---> Delete them?\n",
    "- Electrical one missing value ---> Delete this observation or just leave it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4453c5c",
   "metadata": {},
   "source": [
    "#### Filling missing values for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c90e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical Variables\n",
    "missing_numerical = ['GarageArea', 'GarageCars', 'BsmtFinSF1',\n",
    "                     'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF',\n",
    "                     'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "df[missing_numerical] = df[missing_numerical].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9617aa86",
   "metadata": {},
   "source": [
    "#### Filling special variables\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaf7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling special variables\n",
    "\n",
    "df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(df[\"YearBuilt\"]) \n",
    "# assuming that the garge was bulit with the house \n",
    "\n",
    "\n",
    "df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "most_frequent= df['Electrical'].value_counts().idxmax()\n",
    "df[\"Electrical\"] = df[\"Electrical\"].fillna(most_frequent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data cleaning\n",
    "#df = df.dropna(axis='columns', thresh=1459)\n",
    "#df = df.dropna(axis='rows', how = \"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d436a1c0",
   "metadata": {},
   "source": [
    "## Outliers - To DO -\n",
    "\n",
    "Because regression models are very sensitive to outlier, we need to be aware of them. In the case of categorical data one can use sklearn's `OneHotEncoder` and specify the `min_frequency` parameter. If you specified the min_frequency parameter, rare categorical values will be assigned `infrequend_sklearn`.\n",
    "\n",
    "https://medium.com/owl-analytics/categorical-outliers-dont-exist-8f4e82070cb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7a954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48671e08",
   "metadata": {},
   "source": [
    "## Create New Variables\n",
    "\n",
    "<mark> Copy pastet from here https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> \n",
    "    \n",
    "Should maybe change that and not copy paste it ???\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0931ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totalSqFeet'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "df['totalBathroom'] = df[\"FullBath\"] + df[\"BsmtFullBath\"] + 0.5 * (df[\"HalfBath\"] + df[\"BsmtHalfBath\"])\n",
    "df['houseAge'] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
    "df['reModeled'] = np.where(df[\"YearRemodAdd\"] == df[\"YearBuilt\"], 0, 1)\n",
    "df['isNew'] = np.where(df[\"YrSold\"] == df[\"YearBuilt\"], 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bd0e5",
   "metadata": {},
   "source": [
    "Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b948f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_anymore = ['TotalBsmtSF','1stFlrSF', '2ndFlrSF',\n",
    "                    \"FullBath\", \"BsmtFullBath\", \"HalfBath\",\n",
    "                    \"BsmtHalfBath\", \"YearBuilt\", \"YearRemodAdd\"  ]\n",
    "\n",
    "df= df.drop(not_used_anymore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d126ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653a23d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "### Dealing with Categorical Features (Encoding Categorical Variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71930fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other approach:\n",
    "#to_factor_cols = ['YrSold', 'MoSold', 'MSSubClass']\n",
    "#for col in to_factor_cols:\n",
    "#    X[col] = X[col].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0532dae",
   "metadata": {},
   "source": [
    "## Numerical Features\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4bdf6a",
   "metadata": {},
   "source": [
    "#### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1054fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize data to gain insights (Histograms)\n",
    "df.hist(figsize=(30, 20), bins = 15, edgecolor = 'black', grid = False, color = 'royalblue')\n",
    "plt.suptitle('Histograms of numerical features', x = 0.5, y = 1.02, size = 35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2e5a",
   "metadata": {},
   "source": [
    "#### Top 10 numerical variables highly correlated with `Class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df.corr().Class.sort_values(ascending=False)\n",
    "corr_mat.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1be1f",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "\n",
    "What are the top 10 features selected by Recursive Feature Elimination?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e41f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X interim and response vector y interim\n",
    "X_interim = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_interim , X_test_interim, y_train_interim, y_test_interim = train_test_split(X_interim, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)\n",
    "\n",
    "frames = [X_train_interim, y_train_interim]\n",
    "df_train_interim = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f03244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "selector = rfe.fit(X_train_interim.fillna(0).select_dtypes(exclude='object'), y_train_interim)\n",
    "selectedFeatures = list(\n",
    "    X_interim.select_dtypes(exclude='object').columns[selector.support_])\n",
    "selectedFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458eedf",
   "metadata": {},
   "source": [
    " ??????? Can't be !!!!\n",
    " \n",
    " \n",
    " <mark>Can that be?<mark> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60086c2e",
   "metadata": {},
   "source": [
    "### Overall Quality\n",
    "\n",
    "Overall quality is a very important feature e.g. higher quality houses are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23877520",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Overall Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b9e82",
   "metadata": {},
   "source": [
    "### Living Area\n",
    "\n",
    "The price of a house is linearly correlated with its living area. By examining the scatter plot depicted below, it is evident that there exist some outliers in the data, particularly the two houses positioned in the bottom-right corner. These houses have a living area of more than 4000 square feet but are priced lower than Class 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation: \", df[['GrLivArea','Class']].corr().iloc[1, 0])\n",
    "sns.jointplot(x=df['GrLivArea'],y= df['Class'], kind='reg', marginal_kws={'kde': True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf40709",
   "metadata": {},
   "source": [
    "### GarageCars\n",
    "\n",
    "houses with garage that can hold 4 cars are cheaper than houses with 3 garages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dd51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='GarageCars', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Garage Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a35db40",
   "metadata": {},
   "source": [
    "### House Age\n",
    "\n",
    "In addition to living area, the age of a house also influences its price significantly. Typically, newer houses command higher prices on average. However, it is worth noting that there are some houses constructed before 1900 that have a relatively high price despite their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='houseAge', y='Class', data=df)\n",
    "title = plt.title('House Price/Class by Year Built')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde7a0c",
   "metadata": {},
   "source": [
    "## Lable Encoding ??? \n",
    "\n",
    "Ordinal categorical features are label encoded. ??? <mark> copy pastet from https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> does it even make sense??\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ordinal categorical columns\n",
    "label_encoding_cols = [\n",
    "    \"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"FireplaceQu\", \"Functional\",\n",
    "    \"GarageCond\", \"GarageQual\", \"HeatingQC\", \"KitchenQual\", \"LandSlope\",\n",
    "    \"LotShape\", \"PavedDrive\", \"PoolQC\", \"Street\", \"Utilities\"\n",
    "]\n",
    "\n",
    "# Apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in label_encoding_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039e437",
   "metadata": {},
   "source": [
    "## Asign columns to feature matrix X and response vector y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X and response vector y\n",
    "\n",
    "X = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"] \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0c752",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd005622",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I am not 100% sure about this one!!! ####\n",
    "### Does not change a thing!!!!!!!!!!!!\n",
    "# factorise the binary variables (no need to create two dummy variables)\n",
    "# ---> Problem of Multicollinearity \n",
    "#Without this the get_dummies would create two variables CentralAir_y and CentralAir_n\n",
    "#pd.factorize(X['Street'])\n",
    "# Central Air and one other\n",
    "# does not change a thing\n",
    "# pd.factorize(X['CentralAir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "X = pd.get_dummies(X.iloc[:,:]) \n",
    "\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb459c",
   "metadata": {},
   "source": [
    "## Partitioning of the Data Set Into Train and Test Set\n",
    "\n",
    "We are using a 70/30 (training/testing) splitting. (The parameter `random_state=0` fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb796e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f3886",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector  𝑦 is a binary categorical variable with 25% zeros and 75% ones, `stratify=y` ensures that the random splits have 25% zeros and 75% ones too. Note that `stratify=y` does not mean `stratify=yes` but rather tells the function to take the categorical proportions from response vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552edb67",
   "metadata": {},
   "source": [
    "## ANOVA of Categorical Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019a5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37802d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa35c8e2",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix from train data excluding `Class`\n",
    "\n",
    "#corr_mat = df_train.iloc[:, :-1].corr()\n",
    "#corr_mat = df_train.corr()\n",
    "## difference????\n",
    "corr_mat = X_train.corr()\n",
    "\n",
    "# Select correlations greater than 0.5\n",
    "high_corr_mat = corr_mat[abs(corr_mat) > 0.5]\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(high_corr_mat, annot=True)\n",
    "title = plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5159a9",
   "metadata": {},
   "source": [
    "We can see multicollinearity in our training data. Highly correlated are:\n",
    "\n",
    "- ?\n",
    "- ?\n",
    "- ?\n",
    "- ???\n",
    "\n",
    "#### What is Multicollinearity?\n",
    "\n",
    "Multicollinearity is a situation in which two or more predictor variables in a machine learning model are highly correlated with each other. It is considered bad for several reasons:\n",
    "\n",
    "- It reduces the statistical significance of the coefficients of the correlated variables, making it difficult to interpret the importance of individual predictors in the model.\n",
    "\n",
    "- It can lead to unstable or unreliable estimates of the coefficients, making it hard to predict the effect of changes in the predictor variables on the outcome variable.\n",
    "\n",
    "- It can cause the model to be overfit, meaning it performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "- It can also increase the variance of the coefficients, making the model more sensitive to small changes in the input data.\n",
    "\n",
    "\n",
    "--> Therefore, for each pair of highly correlated features, we will remove a feature that has a lower correlation with `Class`. \n",
    "\n",
    "<mark>TO DO!!<mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fba72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0e0935",
   "metadata": {},
   "source": [
    "## Skewness and Normalizing Variables\n",
    "\n",
    "Linear regression assumes that the data follows a normal distribution, and therefore, transforming skewed data can improve the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=y, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc18e1",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively/right-skewed skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e14ee",
   "metadata": {},
   "source": [
    "To normalize right-skewed data, log transformation can be used as a method since it pulls the larger values towards the center. However, because log(0) results in NaN, log(1+X) is preferred as a fix for the skewness instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d85b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trafo = np.log(1 + y)\n",
    "sns.displot(data=y_trafo, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution Transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness = train_data.skew().sort_values(ascending=False)\n",
    "#skewness[abs(skewness) > 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb137154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of skewed columns\n",
    "#skewed_cols = list(skewness[abs(skewness) > 0.5].index)\n",
    "\n",
    "# Remove 'MSSubClass' and 'SalePrice'\n",
    "#skewed_cols = [\n",
    "#    col for col in skewed_cols if col not in ['MSSubClass', 'SalePrice']\n",
    "#]\n",
    "\n",
    "# Log-transform skewed columns\n",
    "#for col in skewed_cols:\n",
    "#    X[col] = np.log(1 + X[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e87a8d",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e927373",
   "metadata": {},
   "source": [
    "Standardizing the dataset before running machine learning algorithms is generally recommended, except for Decision Tree and Random Forest models. This is because optimization methods and gradient descent algorithms tend to perform and converge faster on features that are similarly scaled.\n",
    "\n",
    "However, outliers can have a negative impact on the sample mean and standard deviation, and models like Lasso and others are highly sensitive to outliers. In such cases, using the median and interquartile range is a better alternative. For this reason, the <mark> RobustScaler?? or StandardScaler? <mark> method is used to transform the training data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9aa1cb",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8d8517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb16554",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc21a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f81de1a",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2051b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10c79d7",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff73fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035401e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5fd53e",
   "metadata": {},
   "source": [
    "## !! UNDER CONSTRUCTION !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_transformer, selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe369a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590209",
   "metadata": {},
   "source": [
    "Now let's see similarly for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__max_depth\": [1, 3, 5, 10],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_selector = selector(dtype_include=object)\n",
    "num_selector = selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    "    encoded_missing_value=-2,\n",
    ")\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n",
    ")\n",
    "\n",
    "#####\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", tree_preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10, 25],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99511e",
   "metadata": {},
   "source": [
    "# Copy Paste from Last Year Function (need to change that)\n",
    "\n",
    "For scoring our models we will be using the weighted  $𝐹_1$\n",
    " -Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e059fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############## Report Functions ##############\n",
    "# Function to get best parametrs, mean cross-validation score of best parameters, standard deviation of the cross-validation scores of the best parameters\n",
    "# and the score of the test set\n",
    "def get_results_cv(func, X_test_cleaned, y_test_cleaned):\n",
    "  \"\"\"\n",
    "  Inputs required: already fitted gridsearcv or randomsearchcv function, cleaned X test set, cleaned y test set\n",
    "  Returns best parameters, mean score, sd of score of cross-validation. Also returns test-score of best parameters\n",
    "  \"\"\"\n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_: .6f}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score: .6f}\")\n",
    "  print(\"Test Score: {:.6f}\".format(func.score(X_test_cleaned, y_test_cleaned)))\n",
    "\n",
    "# Function to get metrics report and heatmap of the confusion matrics for the test set\n",
    "def final_report(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Inputs required: true classes, predicted classes\n",
    "  Returns classification report and confusion matrix of the model\n",
    "  \"\"\"\n",
    "  class_report = metrics.classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  cm = confusion_matrix(y_true, y_pred, normalize = \"all\")\n",
    "  cm = pd.DataFrame(cm, [\"1\", \"2\", \"3\", \"4\", \"5\"],  [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "  plt.figure(figsize = (10,5))\n",
    "  sns.heatmap(cm, annot = True, fmt = \".2%\", cmap = \"Blues\").set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d4d70",
   "metadata": {},
   "source": [
    "# Support Vector Machines (Selina)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baad25c",
   "metadata": {},
   "source": [
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "Support vector machines (SVMs) are a type of supervised learning algorithm that can be utilized for tasks such as classification, regression, and outlier detection. One of the key benefits of SVMs is their effectiveness in handling high-dimensional data, as well as cases where the number of dimensions exceeds the number of samples. Additionally, SVMs are memory-efficient due to their use of a subset of training points, referred to as \"support vectors,\" in the decision-making process. Moreover, SVMs are highly versatile, as they can incorporate different kernel functions to specify the decision function, with the option to define custom kernels. It's important to acknowledge that SVMs come with their own set of limitations. One such limitation arises when dealing with datasets where the number of features far exceeds the number of samples; in such cases, it is essential to exercise caution in selecting kernel functions and regularization terms to prevent over-fitting. Another limitation of SVMs is that they do not offer direct probability estimates, which necessitates the use of a time-consuming five-fold cross-validation method to calculate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6531e7a",
   "metadata": {},
   "source": [
    "## Importing Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0097c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294d3ef",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e399e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SVC Class from Sklearn\n",
    "svm= svm.SVC(\n",
    "        C=1.0,                          # The regularization parameter\n",
    "        kernel='rbf',                   # The kernel type used \n",
    "        degree=3,                       # Degree of polynomial function \n",
    "        gamma='scale',                  # The kernel coefficient\n",
    "        coef0=0.0,                      # If kernel = 'poly'/'sigmoid'\n",
    "        shrinking=True,                 # To use shrinking heuristic\n",
    "        probability=False,              # Enable probability estimates\n",
    "        tol=0.001,                      # Stopping crierion\n",
    "        cache_size=200,                 # Size of kernel cache\n",
    "        class_weight=None,              # The weight of each class\n",
    "        verbose=False,                  # Enable verbose output\n",
    "        max_iter= -1,                   # Hard limit on iterations\n",
    "        decision_function_shape='ovr',  # One-vs-rest or one-vs-one\n",
    "        break_ties=False,               # How to handle breaking ties\n",
    "        random_state=42               # Random state of the model\n",
    "    )\n",
    "\n",
    "print(f\"Parameters of the Support Vector Machine: {svm.get_params().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building and training our model\n",
    "clf = svm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with our data\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b93d0",
   "metadata": {},
   "source": [
    "## Scaling/Pipeline\n",
    "\n",
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "kFold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"SVM\", svm]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 10],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1)\n",
    "\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "get_results_cv(gs, X_test, y_test)\n",
    "y_pred = gs.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "# get test score, metrics report and confusion matrix\n",
    "final_report(y_test, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe73d62",
   "metadata": {},
   "source": [
    "### From Internet\n",
    "\n",
    "https://stackoverflow.com/questions/62346013/svc-object-has-no-attribute-svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = [{'kernel': ['linear', 'poly', 'rbf'],\n",
    "                     'C': [1]}\n",
    "                   ]\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, scoring='accuracy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters set found on development set:\\n\")\n",
    "print(clf.best_params_)\n",
    "print(\"\\nGrid scores on development set:\\n\")\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "\n",
    "print(\"\\nDetailed classification report:\\n\")\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "\n",
    "y_true, y_pred = y_test, clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d935aa",
   "metadata": {},
   "source": [
    "### Random Forest Feature Selection ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm= svm.SVC(random_state = 42, max_iter = 1000)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler],[\"ros\", ros], [\"SVM\", svm]])\n",
    "\n",
    "param_grid= {\n",
    "    \"scaler\": [scaler, mms],\n",
    "    \"ros\": [ros, None],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 10],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train_rf, y_train_cleaned)\n",
    "\n",
    "get_results_cv(gs, X_test_rf, y_test_cleaned)\n",
    "y_pred = gs.best_estimator_.predict(X_test_rf)\n",
    "# get test score, metrics report and confusion matrix\n",
    "final_report(y_test_cleaned, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b35554",
   "metadata": {},
   "source": [
    "### XGBoost Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = svm.SVC(random_state = 42, max_iter = 1000)\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler],[\"ros\", ros], [\"SVM\", svm]])\n",
    "\n",
    "param_grid= {\n",
    "    \"scaler\": [scaler, mms],\n",
    "    \"ros\": [ros, None],\n",
    "    \"SVM__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    \"SVM__C\": [1, 10],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"]\n",
    "}\n",
    "gs = GridSearchCV(estimator = pipe, param_grid = param_grid, scoring = \"f1_weighted\", cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train_xgbc, y_train_cleaned)\n",
    "\n",
    "get_results_cv(gs, X_test_xgbc, y_test_cleaned)\n",
    "y_pred = gs.best_estimator_.predict(X_test_xgbc)\n",
    "# get test score, metrics report and confusion matrix\n",
    "final_report(y_test_cleaned, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab446e3",
   "metadata": {},
   "source": [
    "### PCA Dimension Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9766822",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 42, max_iter = 1000, shrinking = True, kernel = \"sigmoid\", C = 1)\n",
    "pca = PCA()\n",
    "\n",
    "svm_pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"pca\", pca], [\"ros\", ros], [\"SVM\", svm]])\n",
    "param_grid = {\n",
    "    'ros': [ros, None], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"SVM__gamma\": [\"auto\", \"scale\"],\n",
    "    \"pca__n_components\": np.arange(4, 10, 1)\n",
    "}\n",
    "gs = GridSearchCV(estimator = svm_pipe, param_grid = param_grid, scoring = \"f1_weighted\",\n",
    "                  cv = kFold, n_jobs = -1)\n",
    "gs = gs.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "get_results_cv(gs, X_test_cleaned, y_test_cleaned)\n",
    "y_pred = gs.best_estimator_.predict(X_test_cleaned)\n",
    "# get test score, metrics report and confusion matrix\n",
    "final_report(y_test_cleaned, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
