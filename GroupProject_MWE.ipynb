{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# Project ML in Finance Group 5\n",
    "### April 2023\n",
    "\n",
    "\n",
    "#### Cyrill Stoll, Arthur Schlegel, Aleksandar Kuljanin and Selina Waber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66ba9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dean De Cock created the Ames Housing dataset here the link to the [Dataset](https://www.openml.org/search?type=data&sort=runs&id=42165&status=active). This dataset provides information about the sales of residential properties in Ames, Iowa between 2006 and 2010. It consists of 2930 observations and includes a significant amount of explanatory variables, such as 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables, that are used to evaluate the values of homes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d908d81",
   "metadata": {},
   "source": [
    "## Importing Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4cc8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a1591",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894266b",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess missing values\n",
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "print('Percentage of missing values per column:')\n",
    "df_nan.isna().sum() / df_nan.shape[0]\n",
    "\n",
    "\n",
    "# Plot missing values 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a38e",
   "metadata": {},
   "source": [
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the data description.\n",
    "\n",
    "The following variables have NAs that can be filled:\n",
    "\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values for variables where appropriate\n",
    "\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"No\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"No\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"No\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"No\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"No\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"No\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"No\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"No\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"No\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"No\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4eb66",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt.\n",
    "\n",
    "- LotFrontage ---> High Correlation with other variable?\n",
    "- GarageYrBlt can probably be ignored since it highly correlates with YearBuilt.\n",
    "- MasVnrType and MasVnrArea have a strong correaltion with \"YearBuilt\" and \"OverallQual\" ---> Delete them?\n",
    "- Electrical one missing value ---> Delete this observation or just leave it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data cleaning\n",
    "df = df.dropna(axis='columns', thresh=1459)\n",
    "df = df.dropna(axis='rows', how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac133320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653a23d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "### Dealing with Categorical Features (Encoding Categorical Variables) / Splitting Into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X and response vector y\n",
    "X = df.iloc[:, 1:-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd005622",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I am not 100% sure about this one!!! ####\n",
    "### Does not change a thing!!!!!!!!!!!!\n",
    "\n",
    "# factorise the binary variables (no need to create two dummy variables)\n",
    "# ---> Problem of Multicollinearity \n",
    "#Without this the get_dummies would create two variables CentralAir_y and CentralAir_n\n",
    "#pd.factorize(X['Street'])\n",
    "# Central Air and one other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d98380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not change a thing\n",
    "# pd.factorize(X['CentralAir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "# panda way\n",
    "\n",
    "X = pd.get_dummies(X.iloc[:,:]) # not using ID\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113ba34",
   "metadata": {},
   "source": [
    "#### Why does the order of the variables change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ece0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some categorical features as ordered numbers when there is information in the order\n",
    "# see \"A study on Regression applied to the Ames dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb459c",
   "metadata": {},
   "source": [
    "## Partitioning of the Data Set Into Train and Test Set\n",
    "\n",
    "We are using a 70/30 (training/testing) splitting. (The parameter `random_state=0` fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb796e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f3886",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector  𝑦 is a binary categorical variable with 25% zeros and 75% ones, `stratify=y` ensures that the random splits have 25% zeros and 75% ones too. Note that `stratify=y` does not mean `stratify=yes` but rather tells the function to take the categorical proportions from response vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e87a8d",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6bfd4",
   "metadata": {},
   "source": [
    "## Assessing Target Variable \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb468c",
   "metadata": {},
   "source": [
    "** Assess Class imbalance. You make your own assessment on potential effects of class-imbalance. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=y, kind='hist', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc18e1",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef45c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ???????\n",
    "\n",
    "plt.hist(df['Class'], bins=[0, 1, 2, 3, 4, 5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb16554",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc21a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f81de1a",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2051b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10c79d7",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff73fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035401e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5fd53e",
   "metadata": {},
   "source": [
    "## !! UNDER CONSTRUCTION !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_transformer, selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe369a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590209",
   "metadata": {},
   "source": [
    "Now let's see similarly for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__max_depth\": [1, 3, 5, 10],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_selector = selector(dtype_include=object)\n",
    "num_selector = selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    "    encoded_missing_value=-2,\n",
    ")\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n",
    ")\n",
    "\n",
    "#####\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", tree_preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10, 25],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
