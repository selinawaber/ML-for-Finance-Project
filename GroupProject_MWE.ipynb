{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0600489",
   "metadata": {},
   "source": [
    "# Project ML in Finance Group 5\n",
    "### April 2023\n",
    "\n",
    "\n",
    "#### Cyrill Stoll, Arthur Schlegel, Aleksandar Kuljanin and Selina Waber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae66ba9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dean De Cock created the Ames Housing dataset here the link to the [Dataset](https://www.openml.org/search?type=data&sort=runs&id=42165&status=active). This dataset provides information about the sales of residential properties in Ames, Iowa between 2006 and 2010. It consists of 2930 observations and includes a significant amount of explanatory variables, such as 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables, that are used to evaluate the values of homes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d908d81",
   "metadata": {},
   "source": [
    "## Importing Librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b90fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbea4cc8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"GroupProjectDataSet.csv\", sep=',', index_col='Id')\n",
    "print('Shape of data frame:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a1591",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The data set consists of 1460 observations with 81 variables (including the target variable \"(prize) class\" and the id variable). 79 variables are descriptive variables that should explain Class.\n",
    "\n",
    "Quantitative: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold\n",
    "\n",
    "Qualitative: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a84166",
   "metadata": {},
   "outputs": [],
   "source": [
    "numCols = list(df.select_dtypes(exclude='object').columns)\n",
    "print(f\"There are {len(numCols)} numerical features:\\n\", numCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673beb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = list(df.select_dtypes(include='object').columns)\n",
    "print(f\"There are {len(catCols)} categorical features:\\n\", catCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894266b",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "Identifying missing values in data is crucial before determining the appropriate course of action, such as dropping features or imputing missing values, as many machine learning algorithms generate errors when trained on incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a1266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess missing values\n",
    "cols = df.columns[df.isna().any()]\n",
    "df_nan = df[cols].copy()\n",
    "df_nan['Class'] = df['Class']\n",
    "\n",
    "\n",
    "# Plot missing values 2.0\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df_nan.isna().transpose(),\n",
    "            cmap=\"Blues\",\n",
    "            cbar_kws={'label': 'Missing Values'});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aeabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "missing_data.head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660a38e",
   "metadata": {},
   "source": [
    "### Filling missing values for variables where appropriate\n",
    "\n",
    "19 variables have missing values. Of the 19 variables four (PoolQC, MiscFeature, Alley, Fence) have more than 50% missing data and one (FireplaceQu) with nearly 50% missing data. But often NA does not mean that there is no data available. Instead (especially for thecategorical variables) it means that the house is lacking this specific object. NA in the PoolQC variable means that there is no pool; NA in the Alley variable means that there is \"no alley access\". All the descriptions of which NA stand for non-available data and which stand for a missing trait can be found in the data description.\n",
    "\n",
    "\n",
    "<mark> WHY 19 Variables? <mark>\n",
    "\n",
    "The following variables have NAs that can be filled:\n",
    "\n",
    "- PoolQC: Na = No Pool\n",
    "- MiscFeature: Na = None\n",
    "- Alley: NA = No alley access\n",
    "- Fence: NA = No Fence\n",
    "- FireplaceQu: NA = No Fireplace\n",
    "- GarageCond: NA = No Garage\n",
    "- GarageType: NA = No Garage\n",
    "- GarageFinish: NA = No Garage\n",
    "- GarageQual: NA = No Garage\n",
    "- BsmtFinType2: NA = No Basement\n",
    "- BsmtExposure: NA = No Basement\n",
    "- BsmtQual: NA = No Basement\n",
    "- BsmtCond: NA = No Basement\n",
    "- BsmtFinType1: NA = No Basement\n",
    "- MasVnrType: NA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorical Variables \n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(value = \"No\")\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(value = \"No\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(value = \"No\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(value = \"No\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(value = \"No\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(value = \"No\")\n",
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(value = \"No\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(value = \"No\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(value = \"No\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(value = \"No\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(value = \"No\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(value = \"No\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(value = \"No\")\n",
    "df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(value= \"No\") #newly added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab4697",
   "metadata": {},
   "source": [
    "For all but five variables we coud fill the missing data because with them NA indicates the lack of the corresponding trait. For LotFrontage we miss 17% of the values and 5.5% for GarageYrBlt.\n",
    "\n",
    "- LotFrontage ---> High Correlation with other variable?\n",
    "- GarageYrBlt can probably be ignored since it highly correlates with YearBuilt.\n",
    "- MasVnrType and MasVnrArea have a strong correaltion with \"YearBuilt\" and \"OverallQual\" ---> Delete them?\n",
    "- Electrical one missing value ---> Delete this observation or just leave it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce100b7",
   "metadata": {},
   "source": [
    "Filling missing values for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54884ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical Variables\n",
    "missing_numerical = ['GarageArea', 'GarageCars', 'BsmtFinSF1',\n",
    "                     'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF',\n",
    "                     'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']\n",
    "\n",
    "df[missing_numerical] = df[missing_numerical].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ead0a",
   "metadata": {},
   "source": [
    "Filling special variables\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd644a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling special variables\n",
    "\n",
    "df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].fillna(df[\"YearBuilt\"]) \n",
    "# assuming that the garge was bulit with the house \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da218b",
   "metadata": {},
   "source": [
    "Let's check whether there is any missing value left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "missing.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d5bb4",
   "metadata": {},
   "source": [
    "Filling LotFrontage and Electrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either Drop these or fill them\n",
    "\n",
    "df[\"LotFrontage\"] = df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "most_frequent= df['Electrical'].value_counts().idxmax()\n",
    "df[\"Electrical\"] = df[\"Electrical\"].fillna(most_frequent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a71627",
   "metadata": {},
   "source": [
    "#### Further data Cleaning ?\n",
    "\n",
    "<mark> Not needed anymore <mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further data cleaning\n",
    "#df = df.dropna(axis='columns', thresh=1459)\n",
    "#df = df.dropna(axis='rows', how = \"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values for the variables\n",
    "\n",
    "#missing = df.isnull().sum().sort_values(ascending=False)\n",
    "#missing = missing[missing > 0]\n",
    "\n",
    "#percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "#missing_data = pd.concat([missing, percent], axis=1, keys=['Nr. of missing values', 'Share'])\n",
    "#missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c8fa9",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "Because regression models are very sensitive to outlier, we need to be aware of them. Let's examine outliers with a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='GrLivArea', y='Class', data=df) # or df_train ???\n",
    "title = plt.title('House Price vs. Living Area')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d475d",
   "metadata": {},
   "source": [
    "Somehow this graph does not make a lot of sense!!! <mark> Check that <mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1752a",
   "metadata": {},
   "source": [
    "### HOW DO I CHECK FOR OUTLIERS ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do !!\n",
    "plt.boxplot(df[\"Class\"]) # or better df_train?\n",
    "plt.show()\n",
    "plt.hist(df[\"Class\"])\n",
    "plt.show()\n",
    "\n",
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c19d6",
   "metadata": {},
   "source": [
    "<mark>  I guess you want to perform one-hot encoding in a later step. In this case you can use sklearn's OneHotEncoder and specify the min_frequency parameter. If you specified the min_frequency parameter, rare categorical values will be assigned 'infrequend_sklearn'. <mark>\n",
    "    \n",
    "https://medium.com/owl-analytics/categorical-outliers-dont-exist-8f4e82070cb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653a23d",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "### Dealing with Categorical Features (Encoding Categorical Variables) / Splitting Into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cfd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical variables that should be handled as categorical variables\n",
    "df = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n",
    "50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n",
    "80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n",
    "150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}})\n",
    "\n",
    "df = df.replace({\"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n",
    "7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})\n",
    "\n",
    "\n",
    "# other approach\n",
    "####\n",
    "#to_factor_cols = ['YrSold', 'MoSold', 'MSSubClass']\n",
    "\n",
    "#for col in to_factor_cols:\n",
    "#    X[col] = X[col].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb8ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ec2b6",
   "metadata": {},
   "source": [
    "## Numerical Features\n",
    "\n",
    "\n",
    "\n",
    "<mark>Question: Should that not be on df_train instead of just the whole df ??<mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d001997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X interim and response vector y interim\n",
    "\n",
    "X_interim = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_interim , X_test_interim, y_train_interim, y_test_interim = train_test_split(X_interim, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)\n",
    "\n",
    "frames = [X_train_interim, y_train_interim]\n",
    "df_train_interim = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b555f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Visualize data to gain insights (Histograms)\n",
    "# Plot histogram of numerical features using DataFrame.hist(figsize=(30,20)). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_train_interim.hist(figsize=(30, 20), bins = 15, edgecolor = 'black', grid = False, color = 'royalblue')\n",
    "plt.suptitle('Histograms of numerical features', x = 0.5, y = 1.02, size = 35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737d59f",
   "metadata": {},
   "source": [
    "Top 10 numerical variables highly correlated with `Class`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df_train_interim.corr().Class.sort_values(ascending=False)\n",
    "corr_mat.head(11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b7022",
   "metadata": {},
   "source": [
    "What are the top 10 features selected by Recursive Feature Elimination?\n",
    "\n",
    "<mark>Should be X_train and not X ??<mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b23698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=10, step=1)\n",
    "selector = rfe.fit(X_train_interim.fillna(0).select_dtypes(exclude='object'), y_train_interim)\n",
    "selectedFeatures = list(\n",
    "    X_interim.select_dtypes(exclude='object').columns[selector.support_])\n",
    "selectedFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d38793",
   "metadata": {},
   "source": [
    "According to Recursive Feature Elimination, the Overall Quality, Living Area, Number of Full Baths, Size of Garage and Year Built <mark>Can that be?<mark> are some of the most important features in determining house price.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e516986",
   "metadata": {},
   "source": [
    "### Overall Quality\n",
    "\n",
    "Overall quality is a very important feature e.g. higher quality houses are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16940ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='OverallQual', y='Class', data=df_train_interim)\n",
    "title = plt.title('House Price by Overall Quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0c2ac",
   "metadata": {},
   "source": [
    "### Living Area\n",
    "\n",
    "The price of a house is linearly correlated with its living area. By examining the scatter plot depicted below, it is evident that there exist some outliers in the data, particularly the two houses positioned in the bottom-right corner. These houses have a living area of more than 4000 square feet but are priced lower than Class 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Correlation: \", df_train_interim[['GrLivArea','Class']].corr().iloc[1, 0])\n",
    "\n",
    "sns.jointplot(x=df_train_interim['GrLivArea'],y= df_train_interim['Class'], kind='reg', marginal_kws={'kde': True})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65382242",
   "metadata": {},
   "source": [
    "### GarageCars\n",
    "\n",
    "houses with garage that can hold 4 cars are cheaper than houses with 3 garages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='GarageCars', y='Class', data=df_train_interim)\n",
    "title = plt.title('House Price by Garage Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42269f53",
   "metadata": {},
   "source": [
    "### Year Built\n",
    "\n",
    "In addition to living area, the age of a house also influences its price significantly. Typically, newer houses command higher prices on average. However, it is worth noting that there are some houses constructed before 1900 that have a relatively high price despite their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='YearBuilt', y='Class', data=df_train_interim)\n",
    "title = plt.title('House Price by Year Built')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78bd393",
   "metadata": {},
   "source": [
    "## Lable Encoding ??? \n",
    "\n",
    "Ordinal categorical features are label encoded. ??? <mark> copy pastet from https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> does it even make sense??\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ordinal categorical columns\n",
    "label_encoding_cols = [\n",
    "    \"Alley\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"BsmtQual\", \"ExterCond\", \"ExterQual\", \"FireplaceQu\", \"Functional\",\n",
    "    \"GarageCond\", \"GarageQual\", \"HeatingQC\", \"KitchenQual\", \"LandSlope\",\n",
    "    \"LotShape\", \"PavedDrive\", \"PoolQC\", \"Street\", \"Utilities\"\n",
    "]\n",
    "\n",
    "# Apply Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for col in label_encoding_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c62b161",
   "metadata": {},
   "source": [
    "## Create New Variables\n",
    "\n",
    "<mark> Copy pastet from here https://chriskhanhtran.github.io/minimal-portfolio/projects/ames-house-price.html <mark> \n",
    "    \n",
    "Should maybe change that and not copy paste it ???\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a992d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['totalSqFeet'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "df['totalBathroom'] = df[\"FullBath\"] + df[\"BsmtFullBath\"] + 0.5 * (df[\"HalfBath\"] + df[\"BsmtHalfBath\"])\n",
    "df['houseAge'] = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
    "df['reModeled'] = np.where(df[\"YearRemodAdd\"] == df[\"YearBuilt\"], 0, 1)\n",
    "df['isNew'] = np.where(df[\"YrSold\"] == df[\"YearBuilt\"], 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60989c",
   "metadata": {},
   "source": [
    "Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3acd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_anymore = ['TotalBsmtSF','1stFlrSF', '2ndFlrSF',\n",
    "                    \"FullBath\", \"BsmtFullBath\", \"HalfBath\",\n",
    "                    \"BsmtHalfBath\", \"YearBuilt\", \"YearRemodAdd\"  ]\n",
    "\n",
    "df= df.drop(not_used_anymore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0dacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab25074",
   "metadata": {},
   "source": [
    "## Asign columns to feature matrix X and response vector y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asign columns to feature matrix X and response vector y\n",
    "\n",
    "X = df.loc[:, df.columns != \"Class\"]\n",
    "y = df[\"Class\"] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168a457",
   "metadata": {},
   "source": [
    "## Adding Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd005622",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I am not 100% sure about this one!!! ####\n",
    "### Does not change a thing!!!!!!!!!!!!\n",
    "\n",
    "# factorise the binary variables (no need to create two dummy variables)\n",
    "# ---> Problem of Multicollinearity \n",
    "#Without this the get_dummies would create two variables CentralAir_y and CentralAir_n\n",
    "#pd.factorize(X['Street'])\n",
    "# Central Air and one other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d98380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not change a thing\n",
    "# pd.factorize(X['CentralAir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize categorical values, assign output to X\n",
    "# create (multiple) dummy variables for a categorical variable\n",
    "# panda way\n",
    "\n",
    "X = pd.get_dummies(X.iloc[:,:]) # not using ID\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61040da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113ba34",
   "metadata": {},
   "source": [
    "#### Why does the order of the variables change?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2238c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ece0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some categorical features as ordered numbers when there is information in the order\n",
    "# see \"A study on Regression applied to the Ames dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb459c",
   "metadata": {},
   "source": [
    "## Partitioning of the Data Set Into Train and Test Set\n",
    "\n",
    "We are using a 70/30 (training/testing) splitting. (The parameter `random_state=0` fixes the random split in a way such that results are reproducible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb796e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f3886",
   "metadata": {},
   "source": [
    "A stratified sample is one that maintains the proportion of values as in the original data set. If, for example, the response vector  ùë¶ is a binary categorical variable with 25% zeros and 75% ones, `stratify=y` ensures that the random splits have 25% zeros and 75% ones too. Note that `stratify=y` does not mean `stratify=yes` but rather tells the function to take the categorical proportions from response vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f9623",
   "metadata": {},
   "source": [
    "## ANOVA of Categorical Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e96c8d",
   "metadata": {},
   "source": [
    "The house prices in Ames vary significantly across different neighborhoods. Among all the neighborhoods, the median prices of the top three expensive areas - NridgHt, NoRidge, and StoneBr - are roughly  <mark>Class?<mark> which is three times higher than the median prices of the three least expensive neighborhoods - BrDale, DOTRR, and MeadowV. Therefore, there exists a substantial price gap between the most and least expensive neighborhoods in Ames. <mark> This Graph and Text does not make any sense!!! <mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [X_train, y_train]\n",
    "df_train = pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f396a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAKES NO SENSE CHANGE THAT !!!!!!!!!!\n",
    "\n",
    "#ord = df_train.groupby('Neighborhood').median().Class.sort_values(ascending=False).index\n",
    "\n",
    "# Create box plot\n",
    "#sns.boxplot(x='Neighborhood',y='Class', data=df_train, order=ord)\n",
    "#title = plt.title('House Price by Neighborhood')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6cbe6",
   "metadata": {},
   "source": [
    "<mark> Also analyse Roof Material and Kitchen Quality <mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283aa5a",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb35832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix from train data excluding `Class`\n",
    "#corr_mat = df_train.iloc[:, :-1].corr()\n",
    "corr_mat = df_train.corr()\n",
    "\n",
    "# Select correlations greater than 0.5\n",
    "high_corr_mat = corr_mat[abs(corr_mat) > 0.5]\n",
    "\n",
    "# Plot correlation heatmap\n",
    "sns.heatmap(high_corr_mat, annot=True)\n",
    "title = plt.title('Correlation Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b2e1a",
   "metadata": {},
   "source": [
    "We can see multicollinearity in our training data. Highly correlated are:\n",
    "\n",
    "- Fireplaces and GarageArea\n",
    "- ?\n",
    "- ?\n",
    "- ???\n",
    "\n",
    "\n",
    "\n",
    "Multicollinearity is a situation in which two or more predictor variables in a machine learning model are highly correlated with each other. It is considered bad for several reasons:\n",
    "\n",
    "- It reduces the statistical significance of the coefficients of the correlated variables, making it difficult to interpret the importance of individual predictors in the model.\n",
    "\n",
    "- It can lead to unstable or unreliable estimates of the coefficients, making it hard to predict the effect of changes in the predictor variables on the outcome variable.\n",
    "\n",
    "- It can cause the model to be overfit, meaning it performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "- It can also increase the variance of the coefficients, making the model more sensitive to small changes in the input data.\n",
    "\n",
    "\n",
    "<mark> Therefore, for each pair of highly correlated features, we will remove a feature that has a lower correlation with `Class`.<mark> TO DO!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc29ef",
   "metadata": {},
   "source": [
    "<mark> DO AN ANOVA !!!! <mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6bfd4",
   "metadata": {},
   "source": [
    "## Assessing Target Variable \"Class\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb468c",
   "metadata": {},
   "source": [
    "** Assess Class imbalance. You make your own assessment on potential effects of class-imbalance. **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f809249",
   "metadata": {},
   "source": [
    "## Skewness and Normalizing Variables\n",
    "\n",
    "Linear regression assumes that the data follows a normal distribution, and therefore, transforming skewed data can improve the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b93e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1); plt.title('Distribution of Class')\n",
    "sns.histplot(data=y, discrete = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data=y, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc18e1",
   "metadata": {},
   "source": [
    "We see that our \"Class\" deviates from the normal distribution, is positively/right-skewed skewed and shows peakedness (cortosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cc8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: %f\" % df['Class'].skew())\n",
    "print(\"Kurtosis: %f\" % df['Class'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee8612",
   "metadata": {},
   "source": [
    "To normalize right-skewed data, log transformation can be used as a method since it pulls the larger values towards the center. However, because log(0) results in NaN, log(1+X) is preferred as a fix for the skewness instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613dfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trafo = np.log(1 + y)\n",
    "sns.displot(data=y_trafo, kind='hist', kde=True)\n",
    "title = plt.title(\"House Price Distribution Transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness = train_data.skew().sort_values(ascending=False)\n",
    "#skewness[abs(skewness) > 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0fe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of skewed columns\n",
    "#skewed_cols = list(skewness[abs(skewness) > 0.5].index)\n",
    "\n",
    "# Remove 'MSSubClass' and 'SalePrice'\n",
    "#skewed_cols = [\n",
    "#    col for col in skewed_cols if col not in ['MSSubClass', 'SalePrice']\n",
    "#]\n",
    "\n",
    "# Log-transform skewed columns\n",
    "#for col in skewed_cols:\n",
    "#    X[col] = np.log(1 + X[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e87a8d",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f16035",
   "metadata": {},
   "source": [
    "Standardizing the dataset before running machine learning algorithms is generally recommended, except for Decision Tree and Random Forest models. This is because optimization methods and gradient descent algorithms tend to perform and converge faster on features that are similarly scaled.\n",
    "\n",
    "However, outliers can have a negative impact on the sample mean and standard deviation, and models like Lasso and others are highly sensitive to outliers. In such cases, using the median and interquartile range is a better alternative. For this reason, the <mark> RobustScaler?? or StandardScaler? <mark> method is used to transform the training data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79262061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "# Get cols to scale\n",
    "cols_scl = X.columns.values[:]\n",
    "\n",
    "# Apply MinMaxScaler on continuous columns only (check dummies!!!)\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_norm  = mms.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Apply StandardScaler on continuous columns only\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train[cols_scl])  # fit & transform\n",
    "X_test_std  = stdsc.transform(X_test[cols_scl])  # ONLY transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb16554",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc21a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f81de1a",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2051b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a10c79d7",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff73fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035401e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5fd53e",
   "metadata": {},
   "source": [
    "## !! UNDER CONSTRUCTION !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings (related to LogReg 'max_iter' param)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    ")\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_transformer, selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe369a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__C\": [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46590209",
   "metadata": {},
   "source": [
    "Now let's see similarly for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n",
    "    \"classifier__max_depth\": [1, 3, 5, 10],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "cat_selector = selector(dtype_include=object)\n",
    "num_selector = selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    "    encoded_missing_value=-2,\n",
    ")\n",
    "num_tree_processor = SimpleImputer(strategy=\"mean\", add_indicator=True)\n",
    "\n",
    "tree_preprocessor = make_column_transformer(\n",
    "    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector)\n",
    ")\n",
    "\n",
    "#####\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", tree_preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57999e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"classifier__max_depth\": [5, 10, 25],\n",
    "}\n",
    "\n",
    "search_cv = RandomizedSearchCV(clf, param_grid, n_iter=10, random_state=0)\n",
    "\n",
    "search_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print('Best CV accuracy: {:.2f}'.format(search_cv.best_score_))\n",
    "print('Test score:       {:.2f}'.format(search_cv.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(search_cv.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0fe98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
